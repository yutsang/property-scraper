{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import time\n",
    "\n",
    "# Load CSV file\n",
    "df =pd.read_csv('midland_building_list.csv')\n",
    "\n",
    "# Configure Edge WebDriver\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "service = Service(EdgeChromiumDriverManager().install())\n",
    "driver = webdriver.Edge(service=service, options=options)\n",
    "\n",
    "# Detect and replace function for SVG icons\n",
    "def detect_and_replace_svg(icon_url):\n",
    "    if 'PD94bW' in icon_url and \"jUwl\" in icon_url:\n",
    "        return \"Yes\"\n",
    "    elif: \"PD94bW\" in icon_url and \"gNTAi\" in icon_url:\n",
    "        return \"No\"\n",
    "    return \"Unknown\" # Default value if the icon is not detected\n",
    "\n",
    "\n",
    "# Function to handle webscraping\n",
    "def scrape_building_info(row, driver):\n",
    "    try:\n",
    "        url = row['Detail URL'] + \"?lang=english\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        addtional_info_sections = soup.find_all('div', class_='meta-info-container')\n",
    "        \n",
    "        building_info = {\n",
    "            'Building Name': row['Building Name'],\n",
    "            'Address': row['Address'],\n",
    "            'URL': url\n",
    "        }\n",
    "        \n",
    "        for block in addtional_info_sections:\n",
    "            title = block.find('div', class_='title').text.strip() if block.find('div', class_='title') else 'Unknown'\n",
    "            content = block.find('div', class_='content').text.strip() if block.find('div', class_='content') else 'N/A'\n",
    "            if block.find('div', class_='icon'):\n",
    "                icon = block.find('div', class_='icon')['src']\n",
    "                content = detect_and_replace_svg(icon)\n",
    "            building_info[title] = content\n",
    "            \n",
    "        # Append reuslt to the list\n",
    "        return building_info\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured while scraping {row['Building Name']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Process each bulding and save progressively \n",
    "results = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc='Scraping Buildings:'):\n",
    "    result = scrape_building_info(row, driver)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "        pd.DataFrame(results).to_csv('midlandici_building_list_lv2.csv', index=False)\n",
    "        \n",
    "driver.quit() # Close the browser\n",
    "\n",
    "# Save the final result\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv('midlandici_building_list_lv2.csv', index=False)\n",
    "    print(\"Scraping completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting building list scraping...\n",
      "Found 694 buildings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting building list: 100%|██████████| 694/694 [00:12<00:00, 57.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial building list saved\n",
      "\n",
      "Starting detailed information scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings:  91%|█████████▏| 635/694 [34:17<1:30:15, 91.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping 83 Wing Hong Street: Message: timeout: Timed out receiving message from renderer: 298.612\n",
      "  (Session info: chrome=134.0.6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings: 100%|██████████| 694/694 [36:47<00:00,  3.18s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scraped 693/694 buildings\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Automatically installs and initializes ChromeDriver in headless mode\"\"\"\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def detect_and_replace_svg(icon_url):\n",
    "    \"\"\"Convert SVG icons to Yes/No values\"\"\"\n",
    "    if 'PD94bW' in icon_url and \"jUwl\" in icon_url:\n",
    "        return \"Yes\"\n",
    "    elif 'PD94bW' in icon_url and \"gNTAi\" in icon_url:\n",
    "        return \"No\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def scrape_building_info(row, driver):\n",
    "    \"\"\"Scrape detailed information for individual buildings\"\"\"\n",
    "    try:\n",
    "        url = row['Detail URL'] + \"?lang=english\"\n",
    "        driver.get(url)\n",
    "        time.sleep(1.5)  # Reduced sleep for faster scraping\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        building_info = {\n",
    "            'Building Name': row['Building Name'],\n",
    "            'Address': row['Address'],\n",
    "            'URL': url\n",
    "        }\n",
    "        \n",
    "        # Extract meta information blocks\n",
    "        for block in soup.find_all('div', class_='meta-info-container'):\n",
    "            title = block.find('div', class_='title')\n",
    "            content = block.find('div', class_='content')\n",
    "            icon = block.find('div', class_='icon')\n",
    "            \n",
    "            if title and (content or icon):\n",
    "                key = title.text.strip()\n",
    "                value = content.text.strip() if content else detect_and_replace_svg(icon['src'])\n",
    "                building_info[key] = value\n",
    "                \n",
    "        return building_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {row['Building Name']}: {str(e)[:100]}...\")\n",
    "        return None\n",
    "\n",
    "# Phase 1: Scrape building list\n",
    "print(\"Starting building list scraping...\")\n",
    "driver = initialize_driver()\n",
    "\n",
    "\n",
    "# Phase 2: Scrape detailed information\n",
    "print(\"\\nStarting detailed information scraping...\")\n",
    "driver = initialize_driver()\n",
    "df = pd.read_csv(\"midlandici_building_list.csv\")\n",
    "results = []\n",
    "\n",
    "try:\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing buildings\"):\n",
    "        result = scrape_building_info(row, driver)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            # Progressive save every 5 entries\n",
    "            if len(results) % 5 == 0:\n",
    "                pd.DataFrame(results).to_csv(\"midlandici_detailed_info.csv\", index=False)\n",
    "                \n",
    "    # Final save\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(\"midlandici_detailed_info.csv\", index=False)\n",
    "        print(f\"\\nSuccessfully scraped {len(results)}/{len(df)} buildings\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nUser interrupted! Saving current progress...\")\n",
    "    pd.DataFrame(results).to_csv(\"midlandici_detailed_info_PARTIAL.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
