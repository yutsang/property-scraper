{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SCRAPING ESTATE LISTINGS ==========\n",
      "Loaded 178 areas from Centanet_Res_Area_Code.xlsx\n",
      "Removed existing file: 2025-04-02_centanet_estates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing areas: 100%|██████████| 178/178 [1:33:32<00:00, 31.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== SCRAPING ESTATE DETAILS ==========\n",
      "Using input file: 2025-04-02_centanet_estates.csv\n",
      "Loaded 19700 rows from 2025-04-02_centanet_estates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 19700/19700 [29:59:38<00:00,  5.48s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to: 2025-04-02_centanet_estates_scraped.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utility functions\n",
    "def generate_session_id(length=10):\n",
    "    \"\"\"Generate a random session ID consisting of lowercase letters and digits.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    \"\"\"\n",
    "    Clean the subdistrict string to generate a URL-friendly slug.\n",
    "    Any sequence of non-alphanumeric characters is replaced by a hyphen.\n",
    "    The result is lowercased and stripped of extra hyphens.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "def initialize_driver(headless=True):\n",
    "    \"\"\"\n",
    "    Initializes ChromeDriver with custom options.\n",
    "    \n",
    "    Args:\n",
    "        headless (bool): Whether to run Chrome in headless mode\n",
    "        \n",
    "    Returns:\n",
    "        webdriver.Chrome: Initialized Chrome WebDriver\n",
    "    \"\"\"\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.127 Safari/537.36\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "        \n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def random_sleep(min_delay=0, max_delay=2):\n",
    "    \"\"\"Pause execution for a random duration between min_delay and max_delay seconds.\"\"\"\n",
    "    time.sleep(random.uniform(min_delay, max_delay))\n",
    "\n",
    "def scroll_down(driver):\n",
    "    \"\"\"Scrolls down to the bottom of the page to trigger lazy-loaded content.\"\"\"\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    random_sleep()\n",
    "\n",
    "def find_latest_estates_csv():\n",
    "    \"\"\"\n",
    "    Find the most recent CSV file containing estate data.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the latest CSV file, or None if no file is found\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(\"*_centanet_estates.csv\")\n",
    "    date_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})_centanet_estates\\.csv\")\n",
    "    dated_files = [\n",
    "        (pd.to_datetime(match.group(1)), file)\n",
    "        for file in csv_files if (match := date_pattern.search(file))\n",
    "    ]\n",
    "\n",
    "    if not dated_files:\n",
    "        return None\n",
    "\n",
    "    # Select the CSV with the latest date in its filename\n",
    "    latest_date, latest_file = max(dated_files, key=lambda x: x[0])\n",
    "    return latest_file\n",
    "\n",
    "# Function to extract estate listings\n",
    "def extract_estate_data(driver):\n",
    "    \"\"\"\n",
    "    Extracts estate information from the current page.\n",
    "\n",
    "    Returns:\n",
    "        list: List of lists containing estate data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        estate_items = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.property-text.flex.def-property-box\"))\n",
    "        )\n",
    "        for item in estate_items:\n",
    "            try:\n",
    "                # Extract estate link from anchor tag\n",
    "                estate_link = item.get_attribute(\"href\")\n",
    "\n",
    "                # Extract name and address from basic-info section\n",
    "                name = item.find_element(By.CSS_SELECTOR, \"div.main-text\").text.strip()\n",
    "                address = item.find_element(By.CSS_SELECTOR, \"div.address.f-middle\").text.strip()\n",
    "\n",
    "                # Extract other details from basic-data section\n",
    "                blocks = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Block(s)')]/following-sibling::div\").text.strip()\n",
    "                units = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Units')]/following-sibling::div\").text.strip()\n",
    "                unit_rate = item.find_element(By.XPATH, \".//div[contains(text(), 'Unit Rate of Saleable Area')]/following-sibling::div\").text.strip()\n",
    "                mom = item.find_element(By.XPATH, \".//div[contains(text(), 'MoM')]/following-sibling::div\").text.strip()\n",
    "                trans_record = item.find_element(By.XPATH, \".//div[contains(text(), 'Trans. Record')]/following-sibling::div\").text.strip()\n",
    "                for_sale = item.find_element(By.XPATH, \".//div[contains(text(), 'For Sale')]/following-sibling::div\").text.strip()\n",
    "                for_rent = item.find_element(By.XPATH, \".//div[contains(text(), 'For Rent')]/following-sibling::div\").text.strip()\n",
    "\n",
    "                data.append([name, address, blocks, units, unit_rate, mom, trans_record, for_sale, for_rent, estate_link])\n",
    "            except Exception:\n",
    "                continue  # Skip item if any field fails to extract\n",
    "    except Exception:\n",
    "        pass  # Skip page if no estate items found\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scrape_estate_listings(excel_path, output_path=None, limit=None):\n",
    "    \"\"\"\n",
    "    Scrape estate listings from Centanet.\n",
    "    \n",
    "    Args:\n",
    "        excel_path (str): Path to Excel file containing area codes\n",
    "        output_path (str, optional): Path to save CSV output. If None, generates based on date\n",
    "        limit (int, optional): Limit number of areas to scrape. If None, scrapes all\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    # Base URL for the estate listings\n",
    "    base_url = \"https://hk.centanet.com/findproperty/en/list/estate\"\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = f\"{datetime.today().strftime('%Y-%m-%d')}_centanet_estates.csv\"\n",
    "    \n",
    "    # Read area codes from the Excel file\n",
    "    try:\n",
    "        area_df = pd.read_excel(excel_path, engine=\"openpyxl\")\n",
    "        print(f\"Loaded {len(area_df)} areas from {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {excel_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Limit the number of areas if specified\n",
    "    if limit is not None:\n",
    "        area_df = area_df[:limit]\n",
    "        print(f\"Limited to first {limit} areas\")\n",
    "\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    # Remove existing CSV file if exists\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"Removed existing file: {output_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Iterate over each area with a progress bar\n",
    "        for idx, row in tqdm(area_df.iterrows(), total=len(area_df), desc=\"Processing areas\"):\n",
    "            region = row[\"Region\"]\n",
    "            district = row[\"District\"]\n",
    "            subdistrict = row[\"Subdistrict\"]\n",
    "            code = row[\"Code\"]\n",
    "            subdistrict_part = clean_subdistrict(subdistrict)\n",
    "            session_id = generate_session_id()\n",
    "            area_url = f\"{base_url}/{subdistrict_part}_19-{code}?q={session_id}\"\n",
    "            \n",
    "            driver.get(area_url)\n",
    "            #random_sleep()\n",
    "\n",
    "            current_page = 1\n",
    "            area_rows = []\n",
    "            while True:\n",
    "                scroll_down(driver)\n",
    "                page_data = extract_estate_data(driver)\n",
    "                if page_data:\n",
    "                    for row_data in page_data:\n",
    "                        area_rows.append(row_data + [region, district, subdistrict, code])\n",
    "                else:\n",
    "                    break  # Exit loop if no data found on this page\n",
    "\n",
    "                try:\n",
    "                    next_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn-next:not([disabled])\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    random_sleep()\n",
    "                    current_page += 1\n",
    "                except Exception:\n",
    "                    break  # Exit loop if no next page button found\n",
    "\n",
    "            if area_rows:\n",
    "                df = pd.DataFrame(area_rows,\n",
    "                                  columns=[\"Name\", \"Address\", \"Blocks\", \"Units\", \"Unit Rate\", \"MoM\", \"Trans Record\",\n",
    "                                           \"For Sale\", \"For Rent\", \"Estate Link\", \"Region\", \"District\", \"Subdistrict\",\n",
    "                                           \"Code\"])\n",
    "                df.to_csv(output_path, mode=\"a\", index=False, header=not os.path.exists(output_path), encoding=\"utf-8-sig\")\n",
    "            driver.delete_all_cookies()\n",
    "            #random_sleep()\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def scrape_estate_details(input_csv=None, output_csv=None):\n",
    "    \"\"\"\n",
    "    Scrape detailed information for estates in the input CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str, optional): Path to input CSV with estate URLs. If None, finds the latest CSV\n",
    "        output_csv (str, optional): Path to save enriched CSV. If None, generates based on input filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    # Find latest CSV if not provided\n",
    "    if input_csv is None:\n",
    "        input_csv = find_latest_estates_csv()\n",
    "        if input_csv is None:\n",
    "            print(\"No CSV files matching the specified pattern found.\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Using input file: {input_csv}\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_csv is None:\n",
    "        output_csv = input_csv.replace(\"_centanet_estates.csv\", \"_centanet_estates_scraped.csv\")\n",
    "    \n",
    "    # Initialize driver\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    try:\n",
    "        # Read the original CSV\n",
    "        df = pd.read_csv(input_csv)\n",
    "        print(f\"Loaded {len(df)} rows from {input_csv}\")\n",
    "        \n",
    "        # Create new columns for scraped data if they don't already exist\n",
    "        for col in [\"Scraped Estate Name\", \"Occupation Permit\", \"Scraped Blocks\",\n",
    "                    \"Scraped Units\", \"School Net Info\", \"Estate Detailed Address\", \"Developer\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Iterate over each row using tqdm for progress indication\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing URLs\"):\n",
    "            url = row[\"Estate Link\"]\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                random_sleep(1, 3)  # Allow the page to load\n",
    "                scroll_down(driver)  # Scroll to load lazy-loaded content if needed\n",
    "        \n",
    "                # Extract Estate Name\n",
    "                try:\n",
    "                    estate_name_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-title\")\n",
    "                    estate_name = estate_name_elem.text.strip()\n",
    "                except Exception:\n",
    "                    estate_name = None\n",
    "        \n",
    "                # Extract other details: Occupation Permit, Blocks, Units\n",
    "                occupation, blocks_text, units_text = None, None, None\n",
    "                try:\n",
    "                    table_items = driver.find_elements(By.CLASS_NAME, \"table-item\")\n",
    "                    for item in table_items:\n",
    "                        try:\n",
    "                            title_elem = item.find_element(By.CLASS_NAME, \"table-item-title\")\n",
    "                            text_elem = item.find_element(By.CLASS_NAME, \"table-item-text\")\n",
    "                            text_content = text_elem.text.strip()\n",
    "                            if \"Date of Occupation Permit\" in text_content:\n",
    "                                occupation = title_elem.text.strip()\n",
    "                            elif \"No. of Blocks\" in text_content:\n",
    "                                blocks_text = title_elem.text.strip().split()[0]\n",
    "                            elif \"No. of Units\" in text_content:\n",
    "                                units_text = title_elem.text.strip()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract School Net information\n",
    "                school_net_val = None\n",
    "                try:\n",
    "                    items_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in items_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"School Net\" in label_elem.text.strip():\n",
    "                                links_elems = div.find_elements(By.TAG_NAME, \"a\")\n",
    "                                if len(links_elems) >= 2:\n",
    "                                    primary_net = links_elems[0].text.strip()\n",
    "                                    secondary_net = links_elems[1].text.strip()\n",
    "                                    school_net_val = f\"{primary_net} | {secondary_net}\"\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Estate Detailed Address\n",
    "                estate_address = None\n",
    "                try:\n",
    "                    address_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-position\")\n",
    "                    estate_address = address_elem.text.strip()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Developer information\n",
    "                developer_val = None\n",
    "                try:\n",
    "                    developer_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in developer_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"Developer\" in label_elem.text.strip():\n",
    "                                developer_span_elem = div.find_element(By.CLASS_NAME, \"label-item-right\")\n",
    "                                developer_val = developer_span_elem.text.strip()\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Save the scraped data into the DataFrame (for the current row only)\n",
    "                df.at[idx, \"Scraped Estate Name\"] = estate_name\n",
    "                df.at[idx, \"Occupation Permit\"] = occupation\n",
    "                df.at[idx, \"Scraped Blocks\"] = blocks_text\n",
    "                df.at[idx, \"Scraped Units\"] = units_text\n",
    "                df.at[idx, \"School Net Info\"] = school_net_val\n",
    "                df.at[idx, \"Estate Detailed Address\"] = estate_address\n",
    "                df.at[idx, \"Developer\"] = developer_val\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")\n",
    "            \n",
    "            # Write the current DataFrame to CSV to prevent data loss after each iteration\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            \n",
    "            # Pause briefly before processing the next URL\n",
    "            #random_sleep(2, 3)\n",
    "\n",
    "        print(f\"Scraped data saved to: {output_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping estate details: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return output_csv\n",
    "\n",
    "def main(mode=\"both\", excel_path=\"Centanet_Res_Area_Code.xlsx\", listings_csv=None, \n",
    "         details_csv=None, area_limit=None):\n",
    "    \"\"\"\n",
    "    Main function to run both scraping processes.\n",
    "    \n",
    "    Args:\n",
    "        mode (str): \"listings\" to scrape just estate listings, \"details\" to scrape just details,\n",
    "                   \"both\" to scrape both\n",
    "        excel_path (str): Path to Excel file with area codes (needed for listings mode)\n",
    "        listings_csv (str, optional): Path to save listings CSV (if None, auto-generated)\n",
    "        details_csv (str, optional): Path to save details CSV (if None, auto-generated)\n",
    "        area_limit (int, optional): Limit number of areas to scrape (for listings mode)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Paths to created CSV files\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if mode in [\"listings\", \"both\"]:\n",
    "        print(\"========== SCRAPING ESTATE LISTINGS ==========\")\n",
    "        listings_path = scrape_estate_listings(excel_path, listings_csv, area_limit)\n",
    "        results[\"listings_csv\"] = listings_path\n",
    "    \n",
    "    if mode in [\"details\", \"both\"]:\n",
    "        print(\"\\n========== SCRAPING ESTATE DETAILS ==========\")\n",
    "        # If we're in \"both\" mode, use the listings CSV we just created\n",
    "        input_csv = listings_csv if listings_csv else (results.get(\"listings_csv\") if mode == \"both\" else None)\n",
    "        details_path = scrape_estate_details(input_csv, details_csv)\n",
    "        results[\"details_csv\"] = details_path\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Scrape both listings and details, limiting to 5 areas\n",
    "    #main(mode=\"both\", area_limit=5)\n",
    "    main(mode=\"both\")\n",
    "    \n",
    "    # Or just scrape details from an existing CSV\n",
    "    # main(mode=\"details\", listings_csv=\"2023-04-01_centanet_estates.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
