{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SCRAPING ESTATE LISTINGS ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing areas: 100%|██████████| 178/178 [1:24:31<00:00, 28.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created backup: centanet_res_estates_backup.csv\n",
      "\n",
      "========== SCRAPING ESTATE DETAILS ==========\n",
      "Using input file: centanet_res_estates_202504.csv\n",
      "Loaded 19339 rows from centanet_res_estates_202504.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   8%|▊         | 1523/19339 [4:04:43<142:02:01, 28.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL https://hk.centanet.com/estate/en/9-11-Hung-Shing-Street/2-SDBYBPYXPK: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:  15%|█▌        | 2943/19339 [7:49:38<43:36:25,  9.57s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 416\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistings\u001b[39m\u001b[38;5;124m'\u001b[39m: listings_path,\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m'\u001b[39m: details_path\n\u001b[0;32m    413\u001b[0m     }\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 408\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    405\u001b[0m listings_path \u001b[38;5;241m=\u001b[39m scrape_estate_listings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCentanet_Res_Area_Code.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========== SCRAPING ESTATE DETAILS ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m details_path \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_estate_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlistings_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistings\u001b[39m\u001b[38;5;124m'\u001b[39m: listings_path,\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m'\u001b[39m: details_path\n\u001b[0;32m    413\u001b[0m }\n",
      "Cell \u001b[1;32mIn[3], line 292\u001b[0m, in \u001b[0;36mscrape_estate_details\u001b[1;34m(input_csv, output_csv)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m--> 292\u001b[0m     \u001b[43mrandom_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Allow the page to load\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     scroll_down(driver)  \u001b[38;5;66;03m# Scroll to load lazy-loaded content if needed\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# Extract Estate Name\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m, in \u001b[0;36mrandom_sleep\u001b[1;34m(min_d, max_d)\u001b[0m\n\u001b[0;32m     71\u001b[0m min_d \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraperSettings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_delay\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m max_d \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraperSettings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_delay\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_d\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration management\n",
    "CONFIG_FILE = 'scraper_config.ini'\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Initialize and load configuration\"\"\"\n",
    "    config.read(CONFIG_FILE)\n",
    "    \n",
    "    if not config.has_section('FileSettings'):\n",
    "        config.add_section('FileSettings')\n",
    "        config.set('FileSettings', 'base_name', 'centanet_res_estates')\n",
    "        config.set('FileSettings', 'backup_name', 'centanet_res_estates_backup')\n",
    "        config.set('FileSettings', 'update_frequency', 'monthly')\n",
    "        config.set('FileSettings', 'force_update', 'False')\n",
    "        config.set('FileSettings', 'min_file_size_ratio', '0.9')\n",
    "        \n",
    "    if not config.has_section('ScraperSettings'):\n",
    "        config.add_section('ScraperSettings')\n",
    "        config.set('ScraperSettings', 'headless', 'True')\n",
    "        config.set('ScraperSettings', 'min_delay', '0')\n",
    "        config.set('ScraperSettings', 'max_delay', '2')\n",
    "        config.set('ScraperSettings', 'area_limit', 'None')\n",
    "        config.set('ScraperSettings', 'max_retries', '5')\n",
    "\n",
    "    with open(CONFIG_FILE, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "\n",
    "load_config()\n",
    "\n",
    "# Utility functions\n",
    "def generate_session_id(length=10):\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "# nodes.py (updated ChromeDriver configuration)\n",
    "def initialize_driver(params: Dict[str, Any]) -> webdriver.Chrome:\n",
    "    \"\"\"Initialize ChromeDriver with original working configuration\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Essential settings from original working code\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    \n",
    "    # Headless configuration\n",
    "    if params.get('headless', True):\n",
    "        options.add_argument(\"--headless=new\")  # New headless mode\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # Log suppression\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "    \n",
    "    # User agent and compatibility settings\n",
    "    options.add_argument(f\"user-agent={params.get('user_agent', 'Mozilla/5.0')}\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    \n",
    "    # Initialize driver with modern Selenium syntax\n",
    "    service = webdriver.ChromeService(\n",
    "        executable_path=chromedriver_autoinstaller.install()\n",
    "    )\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "def random_sleep(min_d = None, max_d = None):\n",
    "    min_d = config.getint('ScraperSettings', 'min_delay')\n",
    "    max_d = config.getint('ScraperSettings', 'max_delay')\n",
    "    time.sleep(random.uniform(min_d, max_d))\n",
    "\n",
    "def get_current_ym():\n",
    "    return datetime.now().strftime(\"%Y%m\")\n",
    "\n",
    "def get_latest_file():\n",
    "    pattern = f\"{config.get('FileSettings', 'base_name')}_*.csv\"\n",
    "    files = glob.glob(pattern)\n",
    "    return max(files, key=os.path.getctime) if files else None\n",
    "\n",
    "def backup_existing():\n",
    "    latest = get_latest_file()\n",
    "    if not latest:\n",
    "        return\n",
    "    \n",
    "    backup_path = f\"{config.get('FileSettings', 'backup_name')}.csv\"\n",
    "    if os.path.exists(backup_path):\n",
    "        os.remove(backup_path)\n",
    "    os.rename(latest, backup_path)\n",
    "    print(f\"Created backup: {backup_path}\")\n",
    "\n",
    "def should_run_update():\n",
    "    if config.get('FileSettings', 'update_frequency') != 'monthly':\n",
    "        return True\n",
    "        \n",
    "    current_ym = get_current_ym()\n",
    "    latest = get_latest_file()\n",
    "    if not latest:\n",
    "        return True\n",
    "\n",
    "    file_ym = re.search(r'_(\\d{6})\\.csv', latest).group(1)\n",
    "    return file_ym != current_ym or config.getboolean('FileSettings', 'force_update')\n",
    "\n",
    "def validate_update(new_path, backup_path):\n",
    "    if not os.path.exists(backup_path):\n",
    "        return\n",
    "    \n",
    "    new_size = os.path.getsize(new_path)\n",
    "    backup_size = os.path.getsize(backup_path)\n",
    "    \n",
    "    # Check for division by zero\n",
    "    if backup_size == 0:\n",
    "        print(f\"Warning: Backup file is empty.\")\n",
    "        return\n",
    "    \n",
    "    ratio = new_size / backup_size\n",
    "    \n",
    "    if ratio < config.getfloat('FileSettings', 'min_file_size_ratio'):\n",
    "        print(f\"Warning: New file is {ratio:.0%} of backup size. Potential data loss!\")\n",
    "\n",
    "\n",
    "# Core scraping functions\n",
    "def extract_estate_data(driver, existing_links):\n",
    "    data = []\n",
    "    try:\n",
    "        estate_items = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.property-text.flex.def-property-box\"))\n",
    "        )\n",
    "        for item in estate_items:\n",
    "            try:\n",
    "                estate_link = item.get_attribute(\"href\")\n",
    "                if estate_link in existing_links:\n",
    "                    continue\n",
    "                \n",
    "                name = item.find_element(By.CSS_SELECTOR, \"div.main-text\").text.strip()\n",
    "                address = item.find_element(By.CSS_SELECTOR, \"div.address.f-middle\").text.strip()\n",
    "                blocks = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Block(s)')]/following-sibling::div\").text.strip()\n",
    "                units = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Units')]/following-sibling::div\").text.strip()\n",
    "                unit_rate = item.find_element(By.XPATH, \".//div[contains(text(), 'Unit Rate of Saleable Area')]/following-sibling::div\").text.strip()\n",
    "                mom = item.find_element(By.XPATH, \".//div[contains(text(), 'MoM')]/following-sibling::div\").text.strip()\n",
    "                trans_record = item.find_element(By.XPATH, \".//div[contains(text(), 'Trans. Record')]/following-sibling::div\").text.strip()\n",
    "                for_sale = item.find_element(By.XPATH, \".//div[contains(text(), 'For Sale')]/following-sibling::div\").text.strip()\n",
    "                for_rent = item.find_element(By.XPATH, \".//div[contains(text(), 'For Rent')]/following-sibling::div\").text.strip()\n",
    "\n",
    "                data.append([name, address, blocks, units, unit_rate, mom, trans_record, \n",
    "                            for_sale, for_rent, estate_link])\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def scroll_down(driver):\n",
    "    \"\"\"\n",
    "    Scroll to the bottom of the page to load lazy-loaded content.\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: The Selenium WebDriver instance.\n",
    "    \"\"\"\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait for the content to load\n",
    "    random_sleep()\n",
    "\n",
    "def scrape_estate_listings(excel_path):\n",
    "    if not should_run_update():\n",
    "        print(\"Update not required based on configuration\")\n",
    "        return get_latest_file()\n",
    "    \n",
    "    driver = initialize_driver()\n",
    "    existing_links = set()\n",
    "    existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Load existing data\n",
    "    latest_file = get_latest_file()\n",
    "    if latest_file:\n",
    "        existing_data = pd.read_csv(latest_file)\n",
    "        existing_links = set(existing_data['Estate Link'].tolist())\n",
    "    \n",
    "    # Prepare new collection\n",
    "    area_df = pd.read_excel(excel_path, engine=\"openpyxl\")\n",
    "    if config.get('ScraperSettings', 'area_limit') != 'None':\n",
    "        area_limit = config.getint('ScraperSettings', 'area_limit')\n",
    "        area_df = area_df[:area_limit]\n",
    "    \n",
    "    new_data = []\n",
    "    for idx, row in tqdm(area_df.iterrows(), total=len(area_df), desc=\"Processing areas\"):\n",
    "        region = row[\"Region\"]\n",
    "        district = row[\"District\"]\n",
    "        subdistrict = row[\"Subdistrict\"]\n",
    "        code = row[\"Code\"]\n",
    "        \n",
    "        subdistrict_part = clean_subdistrict(subdistrict)\n",
    "        session_id = generate_session_id()\n",
    "        area_url = f\"https://hk.centanet.com/findproperty/en/list/estate/{subdistrict_part}_19-{code}?q={session_id}\"\n",
    "        \n",
    "        driver.get(area_url)\n",
    "        random_sleep()\n",
    "        \n",
    "        current_page = 1\n",
    "        while True:\n",
    "            page_data = extract_estate_data(driver, existing_links)\n",
    "            if not page_data:\n",
    "                break\n",
    "                \n",
    "            # Add regional metadata\n",
    "            new_data.extend([entry + [region, district, subdistrict, code] \n",
    "                           for entry in page_data])\n",
    "            existing_links.update(entry[9] for entry in page_data)\n",
    "            \n",
    "            try:\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn-next:not([disabled])\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                random_sleep()\n",
    "                current_page += 1\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create final dataset\n",
    "    if new_data:\n",
    "        new_df = pd.DataFrame(new_data, columns=[\n",
    "            \"Name\", \"Address\", \"Blocks\", \"Units\", \"Unit Rate\", \"MoM\", \n",
    "            \"Trans Record\", \"For Sale\", \"For Rent\", \"Estate Link\",\n",
    "            \"Region\", \"District\", \"Subdistrict\", \"Code\"\n",
    "        ])\n",
    "        combined_df = pd.concat([existing_data, new_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=['Estate Link'], keep='last', inplace=True)\n",
    "        \n",
    "        # Save results\n",
    "        output_path = f\"{config.get('FileSettings', 'base_name')}_{get_current_ym()}.csv\"\n",
    "        backup_existing()\n",
    "        combined_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        validate_update(output_path, f\"{config.get('FileSettings', 'backup_name')}.csv\")\n",
    "        return output_path\n",
    "    \n",
    "    print(\"No new data found\")\n",
    "    return latest_file\n",
    "\n",
    "# Rest of the functions remain similar with config integration\n",
    "def scrape_estate_details(input_csv=None, output_csv=None):\n",
    "    \"\"\"\n",
    "    Scrape detailed information for estates in the input CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str, optional): Path to input CSV with estate URLs. If None, finds the latest CSV\n",
    "        output_csv (str, optional): Path to save enriched CSV. If None, generates based on input filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    # Find latest CSV if not provided\n",
    "    if input_csv is None:\n",
    "        input_csv = find_latest_estates_csv()\n",
    "        if input_csv is None:\n",
    "            print(\"No CSV files matching the specified pattern found.\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Using input file: {input_csv}\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_csv is None:\n",
    "        output_csv = input_csv.replace(\"_centanet_estates.csv\", \"_centanet_estates_scraped.csv\")\n",
    "    \n",
    "    # Initialize driver\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    try:\n",
    "        # Read the original CSV\n",
    "        df = pd.read_csv(input_csv)\n",
    "        print(f\"Loaded {len(df)} rows from {input_csv}\")\n",
    "        \n",
    "        # Create new columns for scraped data if they don't already exist\n",
    "        for col in [\"Scraped Estate Name\", \"Occupation Permit\", \"Scraped Blocks\",\n",
    "                    \"Scraped Units\", \"School Net Info\", \"Estate Detailed Address\", \"Developer\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Iterate over each row using tqdm for progress indication\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing URLs\"):\n",
    "            url = row[\"Estate Link\"]\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                random_sleep()  # Allow the page to load\n",
    "                scroll_down(driver)  # Scroll to load lazy-loaded content if needed\n",
    "        \n",
    "                # Extract Estate Name\n",
    "                try:\n",
    "                    estate_name_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-title\")\n",
    "                    estate_name = estate_name_elem.text.strip()\n",
    "                except Exception:\n",
    "                    estate_name = None\n",
    "        \n",
    "                # Extract other details: Occupation Permit, Blocks, Units\n",
    "                occupation, blocks_text, units_text = None, None, None\n",
    "                try:\n",
    "                    table_items = driver.find_elements(By.CLASS_NAME, \"table-item\")\n",
    "                    for item in table_items:\n",
    "                        try:\n",
    "                            title_elem = item.find_element(By.CLASS_NAME, \"table-item-title\")\n",
    "                            text_elem = item.find_element(By.CLASS_NAME, \"table-item-text\")\n",
    "                            text_content = text_elem.text.strip()\n",
    "                            if \"Date of Occupation Permit\" in text_content:\n",
    "                                occupation = title_elem.text.strip()\n",
    "                            elif \"No. of Blocks\" in text_content:\n",
    "                                blocks_text = title_elem.text.strip().split()[0]\n",
    "                            elif \"No. of Units\" in text_content:\n",
    "                                units_text = title_elem.text.strip()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract School Net information\n",
    "                school_net_val = None\n",
    "                try:\n",
    "                    items_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in items_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"School Net\" in label_elem.text.strip():\n",
    "                                links_elems = div.find_elements(By.TAG_NAME, \"a\")\n",
    "                                if len(links_elems) >= 2:\n",
    "                                    primary_net = links_elems[0].text.strip()\n",
    "                                    secondary_net = links_elems[1].text.strip()\n",
    "                                    school_net_val = f\"{primary_net} | {secondary_net}\"\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Estate Detailed Address\n",
    "                estate_address = None\n",
    "                try:\n",
    "                    address_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-position\")\n",
    "                    estate_address = address_elem.text.strip()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Developer information\n",
    "                developer_val = None\n",
    "                try:\n",
    "                    developer_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in developer_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"Developer\" in label_elem.text.strip():\n",
    "                                developer_span_elem = div.find_element(By.CLASS_NAME, \"label-item-right\")\n",
    "                                developer_val = developer_span_elem.text.strip()\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Save the scraped data into the DataFrame (for the current row only)\n",
    "                df.at[idx, \"Scraped Estate Name\"] = estate_name\n",
    "                df.at[idx, \"Occupation Permit\"] = occupation\n",
    "                # Assuming blocks_text should be an integer\n",
    "\n",
    "                try:\n",
    "                    df.at[idx, \"Scraped Blocks\"] = int(blocks_text)\n",
    "                except ValueError:\n",
    "                    # Handle the case when blocks_text cannot be converted to an integer\n",
    "                    df.at[idx, \"Scraped Blocks\"] = None  # or any other default value\n",
    "\n",
    "                df.at[idx, \"Scraped Units\"] = units_text\n",
    "                df.at[idx, \"School Net Info\"] = school_net_val\n",
    "                df.at[idx, \"Estate Detailed Address\"] = estate_address\n",
    "                df.at[idx, \"Developer\"] = developer_val\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")\n",
    "            \n",
    "            # Write the current DataFrame to CSV to prevent data loss after each iteration\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            \n",
    "            # Pause briefly before processing the next URL\n",
    "            random_sleep()\n",
    "\n",
    "        print(f\"Scraped data saved to: {output_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping estate details: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return output_csv\n",
    "\n",
    "def main():\n",
    "    if not should_run_update():\n",
    "        print(\"Update not required based on configuration\")\n",
    "        return\n",
    "    \n",
    "    print(\"========== SCRAPING ESTATE LISTINGS ==========\")\n",
    "    listings_path = scrape_estate_listings('Centanet_Res_Area_Code.xlsx')\n",
    "    \n",
    "    print(\"\\n========== SCRAPING ESTATE DETAILS ==========\")\n",
    "    details_path = scrape_estate_details(listings_path)\n",
    "    \n",
    "    return {\n",
    "        'listings': listings_path,\n",
    "        'details': details_path\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
