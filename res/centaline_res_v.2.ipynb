{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SCRAPING ESTATE LISTINGS ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing areas:   3%|â–Ž         | 5/178 [04:17<2:28:37, 51.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== SCRAPING ESTATE DETAILS ==========\n",
      "Using input file: centanet_res_estates_202504.csv\n",
      "Loaded 1235 rows from centanet_res_estates_202504.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 0/1235 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL https://hk.centanet.com/estate/en/The-Merton/2-SSPPWPPYPS: random_sleep() takes 0 positional arguments but 2 were given\n",
      "Error during scraping estate details: random_sleep() takes 0 positional arguments but 2 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import chromedriver_autoinstaller\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration management\n",
    "CONFIG_FILE = 'scraper_config.ini'\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Initialize and load configuration\"\"\"\n",
    "    config.read(CONFIG_FILE)\n",
    "    \n",
    "    if not config.has_section('FileSettings'):\n",
    "        config.add_section('FileSettings')\n",
    "        config.set('FileSettings', 'base_name', 'centanet_res_estates')\n",
    "        config.set('FileSettings', 'backup_name', 'centanet_res_estates_backup')\n",
    "        config.set('FileSettings', 'update_frequency', 'monthly')\n",
    "        config.set('FileSettings', 'force_update', 'False')\n",
    "        config.set('FileSettings', 'min_file_size_ratio', '0.9')\n",
    "        \n",
    "    if not config.has_section('ScraperSettings'):\n",
    "        config.add_section('ScraperSettings')\n",
    "        config.set('ScraperSettings', 'headless', 'True')\n",
    "        config.set('ScraperSettings', 'min_delay', '1')\n",
    "        config.set('ScraperSettings', 'max_delay', '3')\n",
    "        config.set('ScraperSettings', 'area_limit', 'None')\n",
    "        config.set('ScraperSettings', 'max_retries', '3')\n",
    "\n",
    "    with open(CONFIG_FILE, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "\n",
    "load_config()\n",
    "\n",
    "# Utility functions\n",
    "def generate_session_id(length=10):\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def clean_subdistrict(subdistrict):\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]+', '-', subdistrict)\n",
    "    return cleaned.strip('-').lower()\n",
    "\n",
    "def initialize_driver():\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.127 Safari/537.36\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    if config.getboolean('ScraperSettings', 'headless'):\n",
    "        options.add_argument(\"--headless\")\n",
    "        \n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def random_sleep():\n",
    "    min_d = config.getint('ScraperSettings', 'min_delay')\n",
    "    max_d = config.getint('ScraperSettings', 'max_delay')\n",
    "    time.sleep(random.uniform(min_d, max_d))\n",
    "\n",
    "def get_current_ym():\n",
    "    return datetime.now().strftime(\"%Y%m\")\n",
    "\n",
    "def get_latest_file():\n",
    "    pattern = f\"{config.get('FileSettings', 'base_name')}_*.csv\"\n",
    "    files = glob.glob(pattern)\n",
    "    return max(files, key=os.path.getctime) if files else None\n",
    "\n",
    "def backup_existing():\n",
    "    latest = get_latest_file()\n",
    "    if not latest:\n",
    "        return\n",
    "    \n",
    "    backup_path = f\"{config.get('FileSettings', 'backup_name')}.csv\"\n",
    "    if os.path.exists(backup_path):\n",
    "        os.remove(backup_path)\n",
    "    os.rename(latest, backup_path)\n",
    "    print(f\"Created backup: {backup_path}\")\n",
    "\n",
    "def should_run_update():\n",
    "    if config.get('FileSettings', 'update_frequency') != 'monthly':\n",
    "        return True\n",
    "        \n",
    "    current_ym = get_current_ym()\n",
    "    latest = get_latest_file()\n",
    "    if not latest:\n",
    "        return True\n",
    "        \n",
    "    file_ym = re.search(r'_(\\d{6})\\.csv', latest).group(1)\n",
    "    return file_ym != current_ym or config.getboolean('FileSettings', 'force_update')\n",
    "\n",
    "def validate_update(new_path, backup_path):\n",
    "    if not os.path.exists(backup_path):\n",
    "        return\n",
    "    \n",
    "    new_size = os.path.getsize(new_path)\n",
    "    backup_size = os.path.getsize(backup_path)\n",
    "    ratio = new_size / backup_size\n",
    "    \n",
    "    if ratio < config.getfloat('FileSettings', 'min_file_size_ratio'):\n",
    "        print(f\"Warning: New file is {ratio:.0%} of backup size. Potential data loss!\")\n",
    "\n",
    "# Core scraping functions\n",
    "def extract_estate_data(driver, existing_links):\n",
    "    data = []\n",
    "    try:\n",
    "        estate_items = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.property-text.flex.def-property-box\"))\n",
    "        )\n",
    "        for item in estate_items:\n",
    "            try:\n",
    "                estate_link = item.get_attribute(\"href\")\n",
    "                if estate_link in existing_links:\n",
    "                    continue\n",
    "                \n",
    "                name = item.find_element(By.CSS_SELECTOR, \"div.main-text\").text.strip()\n",
    "                address = item.find_element(By.CSS_SELECTOR, \"div.address.f-middle\").text.strip()\n",
    "                blocks = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Block(s)')]/following-sibling::div\").text.strip()\n",
    "                units = item.find_element(By.XPATH, \".//div[contains(text(), 'No. of Units')]/following-sibling::div\").text.strip()\n",
    "                unit_rate = item.find_element(By.XPATH, \".//div[contains(text(), 'Unit Rate of Saleable Area')]/following-sibling::div\").text.strip()\n",
    "                mom = item.find_element(By.XPATH, \".//div[contains(text(), 'MoM')]/following-sibling::div\").text.strip()\n",
    "                trans_record = item.find_element(By.XPATH, \".//div[contains(text(), 'Trans. Record')]/following-sibling::div\").text.strip()\n",
    "                for_sale = item.find_element(By.XPATH, \".//div[contains(text(), 'For Sale')]/following-sibling::div\").text.strip()\n",
    "                for_rent = item.find_element(By.XPATH, \".//div[contains(text(), 'For Rent')]/following-sibling::div\").text.strip()\n",
    "\n",
    "                data.append([name, address, blocks, units, unit_rate, mom, trans_record, \n",
    "                            for_sale, for_rent, estate_link])\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def scrape_estate_listings(excel_path):\n",
    "    if not should_run_update():\n",
    "        print(\"Update not required based on configuration\")\n",
    "        return get_latest_file()\n",
    "    \n",
    "    driver = initialize_driver()\n",
    "    existing_links = set()\n",
    "    existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Load existing data\n",
    "    latest_file = get_latest_file()\n",
    "    if latest_file:\n",
    "        existing_data = pd.read_csv(latest_file)\n",
    "        existing_links = set(existing_data['Estate Link'].tolist())\n",
    "    \n",
    "    # Prepare new collection\n",
    "    area_df = pd.read_excel(excel_path, engine=\"openpyxl\")\n",
    "    if config.get('ScraperSettings', 'area_limit') != 'None':\n",
    "        area_limit = config.getint('ScraperSettings', 'area_limit')\n",
    "        area_df = area_df[:area_limit]\n",
    "    \n",
    "    new_data = []\n",
    "    for idx, row in tqdm(area_df[:5].iterrows(), total=len(area_df), desc=\"Processing areas\"):\n",
    "        region = row[\"Region\"]\n",
    "        district = row[\"District\"]\n",
    "        subdistrict = row[\"Subdistrict\"]\n",
    "        code = row[\"Code\"]\n",
    "        \n",
    "        subdistrict_part = clean_subdistrict(subdistrict)\n",
    "        session_id = generate_session_id()\n",
    "        area_url = f\"https://hk.centanet.com/findproperty/en/list/estate/{subdistrict_part}_19-{code}?q={session_id}\"\n",
    "        \n",
    "        driver.get(area_url)\n",
    "        random_sleep()\n",
    "        \n",
    "        current_page = 1\n",
    "        while True:\n",
    "            page_data = extract_estate_data(driver, existing_links)\n",
    "            if not page_data:\n",
    "                break\n",
    "                \n",
    "            # Add regional metadata\n",
    "            new_data.extend([entry + [region, district, subdistrict, code] \n",
    "                           for entry in page_data])\n",
    "            existing_links.update(entry[9] for entry in page_data)\n",
    "            \n",
    "            try:\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn-next:not([disabled])\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                random_sleep()\n",
    "                current_page += 1\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create final dataset\n",
    "    if new_data:\n",
    "        new_df = pd.DataFrame(new_data, columns=[\n",
    "            \"Name\", \"Address\", \"Blocks\", \"Units\", \"Unit Rate\", \"MoM\", \n",
    "            \"Trans Record\", \"For Sale\", \"For Rent\", \"Estate Link\",\n",
    "            \"Region\", \"District\", \"Subdistrict\", \"Code\"\n",
    "        ])\n",
    "        combined_df = pd.concat([existing_data, new_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=['Estate Link'], keep='last', inplace=True)\n",
    "        \n",
    "        # Save results\n",
    "        output_path = f\"{config.get('FileSettings', 'base_name')}_{get_current_ym()}.csv\"\n",
    "        backup_existing()\n",
    "        combined_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        validate_update(output_path, f\"{config.get('FileSettings', 'backup_name')}.csv\")\n",
    "        return output_path\n",
    "    \n",
    "    print(\"No new data found\")\n",
    "    return latest_file\n",
    "\n",
    "# Rest of the functions remain similar with config integration\n",
    "def scrape_estate_details(input_csv=None, output_csv=None):\n",
    "    \"\"\"\n",
    "    Scrape detailed information for estates in the input CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str, optional): Path to input CSV with estate URLs. If None, finds the latest CSV\n",
    "        output_csv (str, optional): Path to save enriched CSV. If None, generates based on input filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    # Find latest CSV if not provided\n",
    "    if input_csv is None:\n",
    "        input_csv = find_latest_estates_csv()\n",
    "        if input_csv is None:\n",
    "            print(\"No CSV files matching the specified pattern found.\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Using input file: {input_csv}\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_csv is None:\n",
    "        output_csv = input_csv.replace(\"_centanet_estates.csv\", \"_centanet_estates_scraped.csv\")\n",
    "    \n",
    "    # Initialize driver\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    try:\n",
    "        # Read the original CSV\n",
    "        df = pd.read_csv(input_csv)\n",
    "        print(f\"Loaded {len(df)} rows from {input_csv}\")\n",
    "        \n",
    "        # Create new columns for scraped data if they don't already exist\n",
    "        for col in [\"Scraped Estate Name\", \"Occupation Permit\", \"Scraped Blocks\",\n",
    "                    \"Scraped Units\", \"School Net Info\", \"Estate Detailed Address\", \"Developer\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Iterate over each row using tqdm for progress indication\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing URLs\"):\n",
    "            url = row[\"Estate Link\"]\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                random_sleep(2, 3)  # Allow the page to load\n",
    "                scroll_down(driver)  # Scroll to load lazy-loaded content if needed\n",
    "        \n",
    "                # Extract Estate Name\n",
    "                try:\n",
    "                    estate_name_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-title\")\n",
    "                    estate_name = estate_name_elem.text.strip()\n",
    "                except Exception:\n",
    "                    estate_name = None\n",
    "        \n",
    "                # Extract other details: Occupation Permit, Blocks, Units\n",
    "                occupation, blocks_text, units_text = None, None, None\n",
    "                try:\n",
    "                    table_items = driver.find_elements(By.CLASS_NAME, \"table-item\")\n",
    "                    for item in table_items:\n",
    "                        try:\n",
    "                            title_elem = item.find_element(By.CLASS_NAME, \"table-item-title\")\n",
    "                            text_elem = item.find_element(By.CLASS_NAME, \"table-item-text\")\n",
    "                            text_content = text_elem.text.strip()\n",
    "                            if \"Date of Occupation Permit\" in text_content:\n",
    "                                occupation = title_elem.text.strip()\n",
    "                            elif \"No. of Blocks\" in text_content:\n",
    "                                blocks_text = title_elem.text.strip().split()[0]\n",
    "                            elif \"No. of Units\" in text_content:\n",
    "                                units_text = title_elem.text.strip()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract School Net information\n",
    "                school_net_val = None\n",
    "                try:\n",
    "                    items_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in items_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"School Net\" in label_elem.text.strip():\n",
    "                                links_elems = div.find_elements(By.TAG_NAME, \"a\")\n",
    "                                if len(links_elems) >= 2:\n",
    "                                    primary_net = links_elems[0].text.strip()\n",
    "                                    secondary_net = links_elems[1].text.strip()\n",
    "                                    school_net_val = f\"{primary_net} | {secondary_net}\"\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Estate Detailed Address\n",
    "                estate_address = None\n",
    "                try:\n",
    "                    address_elem = driver.find_element(By.CLASS_NAME, \"estate-detail-banner-position\")\n",
    "                    estate_address = address_elem.text.strip()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Extract Developer information\n",
    "                developer_val = None\n",
    "                try:\n",
    "                    developer_divs = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "                    for div in developer_divs:\n",
    "                        try:\n",
    "                            label_elem = div.find_element(By.CLASS_NAME, \"label-item-left\")\n",
    "                            if \"Developer\" in label_elem.text.strip():\n",
    "                                developer_span_elem = div.find_element(By.CLASS_NAME, \"label-item-right\")\n",
    "                                developer_val = developer_span_elem.text.strip()\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Save the scraped data into the DataFrame (for the current row only)\n",
    "                df.at[idx, \"Scraped Estate Name\"] = estate_name\n",
    "                df.at[idx, \"Occupation Permit\"] = occupation\n",
    "                df.at[idx, \"Scraped Blocks\"] = blocks_text\n",
    "                df.at[idx, \"Scraped Units\"] = units_text\n",
    "                df.at[idx, \"School Net Info\"] = school_net_val\n",
    "                df.at[idx, \"Estate Detailed Address\"] = estate_address\n",
    "                df.at[idx, \"Developer\"] = developer_val\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")\n",
    "            \n",
    "            # Write the current DataFrame to CSV to prevent data loss after each iteration\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            \n",
    "            # Pause briefly before processing the next URL\n",
    "            random_sleep(2, 3)\n",
    "\n",
    "        print(f\"Scraped data saved to: {output_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping estate details: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return output_csv\n",
    "\n",
    "def main():\n",
    "    if not should_run_update():\n",
    "        print(\"Update not required based on configuration\")\n",
    "        return\n",
    "    \n",
    "    print(\"========== SCRAPING ESTATE LISTINGS ==========\")\n",
    "    listings_path = scrape_estate_listings('Centanet_Res_Area_Code.xlsx')\n",
    "    \n",
    "    print(\"\\n========== SCRAPING ESTATE DETAILS ==========\")\n",
    "    details_path = scrape_estate_details(listings_path)\n",
    "    \n",
    "    return {\n",
    "        'listings': listings_path,\n",
    "        'details': details_path\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
