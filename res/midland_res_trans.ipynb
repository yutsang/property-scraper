{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Midland Residential Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping district IDs::  84%|████████▍ | 109/130 [17:04<04:19, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while scraping district ID 130ND30023, page 4: 504 Server Error: Gateway Time-out for url: https://data.midland.com.hk/search/v2/transactions?ad=true&chart=true&lang=en&currency=HKD&unit=feet&search_behavior=normal&tx_date=3year&limit=1000&intsmdist_ids=130ND30023&page=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping district IDs:: 100%|██████████| 130/130 [20:24<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved (201135 records) to midland_res_transaction_data.csv\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import glob \n",
    "from datetime import datetime\n",
    "\n",
    "# Define a function to handle web requests \n",
    "def get_soup(url, params=None):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 \\\n",
    "            Satari/537.36 Edg/131.0.0.0', \n",
    "        'authorization':'''Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJndWlkIjoibXItMjAyNC0xMi0wNy0tLVlWN0hKU2QxelRzOHpwVDhJNEdjdGxLcjQ1Z0l4cWhsdVp3SEdvZXVSX1o3RkU2cmh1Q1NjVVpqM1E3SXIzZWVQSmZpMy1JSSIsImF1ZCI6Im15cGFnZWFwcC1tbm5rYiIsInN1YiI6Im1yLTIwMjQtMTItMDctLS1ZVjdISlNkMXpUczh6cFQ4STRHY3RsS3I0NWdJeHFobHVad0hHb2V1Ul9aN0ZFNnJodUNTY1VaajNRN0lyM2VlUEpmaTMtSUkiLCJpYXQiOjE3MzM1NDk0MjUsImV4cCI6MTc2ODEwOTQyNSwiaXNzIjoiZGF0YS5taWRsYW5kLmNvbS5oayJ9.LOOVgc_Nw7OPNnAlB8iC1kRHL0W8UVNVa0GaJYaxTxVZtO33ZbkR64rxMHSifvZOzYr38aJENj-SDIbkq4Y75CxqMPegyBUgHtaub-Fez5qaH2W0Dz71pUdYijDG3rB4Dkbdf8k21QsHerJmOFnpryzTVnZDxv-3g8Lmjz2WUhmrqMamKox3w-T9wRJ4p_wzcJwvXWgtvxkapr3Ep0YSJy3fJsV-Nwm_QiJf2JR0V4rOAu7f-YLMSy7IYje3W-HvVqAZV2cDphg_cYnf6CpirJPu_ix2z6BtIMpYMXeSiZyZtKCHiWFNtUm6QTD2adArWtLl_NvbgcH9mhVYuWi8NcrZBdBh4c72bSNRm104oEbRb9-vb1AylH2oFkEz33xXXEAJRtbQxoQ3qZj_yoDIexrinOSlkJB50fSu98Xizv9eZstnbtzkgVjfKpOAWQFdHKennjN9Azq6yTlejDVspL7A0JsY4ZlO4HQNdkNhiOQDYypHgx8jQMm0B0rbaa0cEz1S0s43Lh01eNVBN9Is35jAWFsJIP-iLvHqXJ9d0pGoHe0N7PQk2dmLo9E5szP0U04MZxt4m9TEpJkn-0uS_ZDSVABlBU2KGIkTmuzm1VltsDhPhoNrbJBJVdxJJdublpDnVFk8aO1gFWKNzptw48ipmLfpRosynC_x3Ud6QMU'''\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status() # Will raise HTTPError for bad requests (4XX or 5XX)\n",
    "    #return BeautifulSoup(response.text, 'html.parser')\n",
    "    return response.json()\n",
    "\n",
    "# Define a field map for nested fields\n",
    "FIELD_MAP = {\n",
    "    'id': ['id'],\n",
    "    'region_id': ['region', 'id'],\n",
    "    'region_name': ['region', 'name'],\n",
    "    'subregion_id': ['subregion', 'id'],\n",
    "    'subregion': ['subregion', 'name'],\n",
    "    'district_id': ['district', 'id'],\n",
    "    'district': ['district', 'name'],\n",
    "    'sm_district_id': ['sm_district', 'id'],\n",
    "    'sm_district': ['sm_district', 'name'],\n",
    "    'combined_district_id': ['combined_district', 'id'],\n",
    "    'combined_district': ['combined_district', 'name'],\n",
    "    'int_district_id': ['int_district', 'id'],\n",
    "    'int_district': ['int_district', 'name'],\n",
    "    'int_sm_district_id': ['int_sm_district', 'id'],\n",
    "    'int_sm_district': ['int_sm_district', 'name'],\n",
    "    'estate_id': ['estate', 'id'],\n",
    "    'estate': ['estate', 'name'],\n",
    "    'building_id': ['building', 'id'],\n",
    "    'building': ['building', 'name'],\n",
    "    'building_first_op_date': ['building', 'first_op_date'],\n",
    "    'unit': ['unit', 'id'],\n",
    "    'floor': ['floor'],\n",
    "    'floor_level': ['floor_level', 'name'],\n",
    "    'floor_level_id': ['floor_level', 'id'],\n",
    "    'flat': ['flat'],\n",
    "    'area': ['area'],\n",
    "    'net_area': ['net_area'],\n",
    "    'price': ['price'],\n",
    "    'tags': ['tags'],\n",
    "    'tx_date': ['tx_date'],\n",
    "    'tx_type': ['tx_type'],\n",
    "    'last_tx_date': ['last_tx_date'],\n",
    "    'holding_period': ['holding_period'],\n",
    "    'last_price': ['last_price'],\n",
    "    'mkt_type': ['mkt_type'],\n",
    "    'source': ['source'],\n",
    "    'original_source': ['original_source'],\n",
    "    'update_date': ['update_date'],\n",
    "    'gain': ['gain'],\n",
    "    'transaction_type': ['transaction_type'],\n",
    "    'url_desc': ['url_desc'],\n",
    "    'location': ['location']\n",
    "}\n",
    "    \n",
    "# Function to extract fields based on a mapping\n",
    "def extract_fields(result, field_map):\n",
    "    row = {}\n",
    "    for key, path in field_map.items():\n",
    "        temp = result\n",
    "        for p in path:\n",
    "            temp = temp.get(p, None)\n",
    "            if temp is None:  # Stop if any part of the path is missing\n",
    "                break\n",
    "        row[key] = temp\n",
    "    return row\n",
    "\n",
    "\n",
    "def scrape_data(district_ids, base_url, params):\n",
    "    all_data = []\n",
    "    for district_id in tqdm(district_ids, desc='Scraping district IDs:'):\n",
    "        params['intsmdist_ids'] = district_id\n",
    "        page = 1\n",
    "        while True:\n",
    "            params['page'] = page\n",
    "            try:\n",
    "                data = get_soup(base_url, params)  # Use updated get_soup function\n",
    "                #print(f\"Scraping page {page} for district ID {district_id}\")\n",
    "                \n",
    "                results = data.get('result', [])\n",
    "                if not results:\n",
    "                    #print(f\"No more results found for district ID {district_id} on page {page}\")\n",
    "                    break\n",
    "                \n",
    "                for result in results:\n",
    "                    row = extract_fields(result, FIELD_MAP)\n",
    "                    if any(row.values()):\n",
    "                        all_data.append(row)\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(3)  # Avoid rate-limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while scraping district ID {district_id}, page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f'Data saved ({len(data)} records) to {filename}')\n",
    "        \n",
    "# Read the district IDs from the file\n",
    "def read_district_ids_from_excel(filename, column_name):\n",
    "    df = pd.read_excel(filename, usecols=[column_name])\n",
    "    return df[column_name].dropna().unique().tolist()\n",
    "\n",
    "# Script Execution \n",
    "base_url = \"https://data.midland.com.hk/search/v2/transactions\"\n",
    "params = {\n",
    "    \"ad\": \"true\",\n",
    "    \"chart\": \"true\",\n",
    "    \"lang\": \"en\",\n",
    "    \"currency\": \"HKD\",\n",
    "    \"unit\": \"feet\",\n",
    "    \"search_behavior\": \"normal\",\n",
    "    \"tx_date\": \"3year\",\n",
    "    \"limit\": 1000  # Number of records per page\n",
    "}\n",
    "\n",
    "# Read district IDs from Excel file (ensure file exists)\n",
    "district_ids = read_district_ids_from_excel('midland_res_area_code.xlsx', 'm_idstrict_code')\n",
    "\n",
    "# Scrape data and save to CSV\n",
    "scraped_data = scrape_data(district_ids, base_url, params)\n",
    "save_to_csv(scraped_data, 'midland_res_transaction_data.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
