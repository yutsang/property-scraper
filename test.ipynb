{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fa80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.config import OmegaConfigLoader\n",
    "\n",
    "loader = OmegaConfigLoader(conf_source=\"conf/base\")\n",
    "params = loader[\"parameters\"]\n",
    "print(params[\"Control_date\"][\"centaline_estates\"])  # Should output \"2025-04-09\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47602fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 2025-04-02_centanet_estates_scraped.csv to estate_details_raw.parquet\n"
     ]
    }
   ],
   "source": [
    "# Change CSV to Parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def csv_to_parquet(csv_file_path, parquet_file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Write the DataFrame to a Parquet file\n",
    "        df.to_parquet(parquet_file_path, index=False)\n",
    "        \n",
    "        print(f\"Successfully converted {csv_file_path} to {parquet_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = '2025-04-02_centanet_estates.csv'\n",
    "parquet_file_path = 'estate_listings.parquet'\n",
    "\n",
    "# Ensure pandas is installed\n",
    "# pip install pandas\n",
    "\n",
    "csv_to_parquet(csv_file_path, parquet_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f31b01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Blocks</th>\n",
       "      <th>Units</th>\n",
       "      <th>UnitRate</th>\n",
       "      <th>MoM</th>\n",
       "      <th>ForSale</th>\n",
       "      <th>ForRent</th>\n",
       "      <th>Link</th>\n",
       "      <th>Region</th>\n",
       "      <th>District</th>\n",
       "      <th>Subdistrict</th>\n",
       "      <th>Code</th>\n",
       "      <th>LastScraped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Address, Blocks, Units, UnitRate, MoM, ForSale, ForRent, Link, Region, District, Subdistrict, Code, LastScraped]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = 'data/02_intermediate/centaline_estate_lv_1.parquet'\n",
    "pq_file = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "pq_file[pq_file['Name']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22191c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a517d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_trans</th>\n",
       "      <th>region_id_trans</th>\n",
       "      <th>region_name_trans</th>\n",
       "      <th>subregion_id_trans</th>\n",
       "      <th>subregion</th>\n",
       "      <th>district_id_trans</th>\n",
       "      <th>district</th>\n",
       "      <th>sm_district_id_trans</th>\n",
       "      <th>sm_district</th>\n",
       "      <th>combined_district_id_trans</th>\n",
       "      <th>...</th>\n",
       "      <th>market_stat_monthly_10_date</th>\n",
       "      <th>market_stat_monthly_10_avg_net_ft_price</th>\n",
       "      <th>market_stat_monthly_11_date</th>\n",
       "      <th>market_stat_monthly_11_avg_net_ft_price</th>\n",
       "      <th>index_component_estate_net_ft_price</th>\n",
       "      <th>index_component_estate_net_ft_price_chg</th>\n",
       "      <th>parent_estate_id</th>\n",
       "      <th>parent_estate_name</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>price_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I20250301813</td>\n",
       "      <td>10</td>\n",
       "      <td>Hong Kong Island</td>\n",
       "      <td>1001</td>\n",
       "      <td>Central And Western</td>\n",
       "      <td>100103</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>10010001</td>\n",
       "      <td>Kennedy Town</td>\n",
       "      <td>100103-10010001</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>19933.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>19478.0</td>\n",
       "      <td>19449.0</td>\n",
       "      <td>-0.025702</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>48.834628</td>\n",
       "      <td>-20122.165372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I20250301813</td>\n",
       "      <td>10</td>\n",
       "      <td>Hong Kong Island</td>\n",
       "      <td>1001</td>\n",
       "      <td>Central And Western</td>\n",
       "      <td>100103</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>10010001</td>\n",
       "      <td>Kennedy Town</td>\n",
       "      <td>100103-10010001</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>19933.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>19478.0</td>\n",
       "      <td>19449.0</td>\n",
       "      <td>-0.025702</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>48.834628</td>\n",
       "      <td>-20122.165372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I20250302773</td>\n",
       "      <td>10</td>\n",
       "      <td>Hong Kong Island</td>\n",
       "      <td>1001</td>\n",
       "      <td>Central And Western</td>\n",
       "      <td>100103</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>10010005</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>100103-10010005</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>15753.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>15753.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>49.315068</td>\n",
       "      <td>-15703.684932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I20250400203</td>\n",
       "      <td>10</td>\n",
       "      <td>Hong Kong Island</td>\n",
       "      <td>1001</td>\n",
       "      <td>Central And Western</td>\n",
       "      <td>100103</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>10010006</td>\n",
       "      <td>Mid-Levels Central</td>\n",
       "      <td>100103-10010006</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>16334.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>16334.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18372.703412</td>\n",
       "      <td>611.703412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I20250301892</td>\n",
       "      <td>10</td>\n",
       "      <td>Hong Kong Island</td>\n",
       "      <td>1001</td>\n",
       "      <td>Central And Western</td>\n",
       "      <td>100103</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>10010005</td>\n",
       "      <td>Mid-Levels West</td>\n",
       "      <td>100103-10010005</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>22373.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>22373.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18118.811881</td>\n",
       "      <td>-0.188119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234138</th>\n",
       "      <td>NO2021050521050501840081</td>\n",
       "      <td>30</td>\n",
       "      <td>New Territories</td>\n",
       "      <td>3018</td>\n",
       "      <td>Islands</td>\n",
       "      <td>301803</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>30180002</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>301803-30180002</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>11083.0</td>\n",
       "      <td>11052.0</td>\n",
       "      <td>-3.711448</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12808.022923</td>\n",
       "      <td>1460.022923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234139</th>\n",
       "      <td>NO2021050421050401950028</td>\n",
       "      <td>30</td>\n",
       "      <td>New Territories</td>\n",
       "      <td>3018</td>\n",
       "      <td>Islands</td>\n",
       "      <td>301803</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>30180002</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>301803-30180002</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>11083.0</td>\n",
       "      <td>11052.0</td>\n",
       "      <td>-3.711448</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>15463.917526</td>\n",
       "      <td>4115.917526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234140</th>\n",
       "      <td>NO2021050321050301110058</td>\n",
       "      <td>30</td>\n",
       "      <td>New Territories</td>\n",
       "      <td>3018</td>\n",
       "      <td>Islands</td>\n",
       "      <td>301803</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>30180002</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>301803-30180002</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>10644.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>11353.0</td>\n",
       "      <td>11127.0</td>\n",
       "      <td>-0.233121</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13141.524106</td>\n",
       "      <td>1833.524106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234141</th>\n",
       "      <td>NO2021043021043002190055</td>\n",
       "      <td>30</td>\n",
       "      <td>New Territories</td>\n",
       "      <td>3018</td>\n",
       "      <td>Islands</td>\n",
       "      <td>301803</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>30180002</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>301803-30180002</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>10644.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>11353.0</td>\n",
       "      <td>11127.0</td>\n",
       "      <td>-0.233121</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12954.898911</td>\n",
       "      <td>1646.898911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234142</th>\n",
       "      <td>NO2021043021043000550128</td>\n",
       "      <td>30</td>\n",
       "      <td>New Territories</td>\n",
       "      <td>3018</td>\n",
       "      <td>Islands</td>\n",
       "      <td>301803</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>30180002</td>\n",
       "      <td>Tung Chung</td>\n",
       "      <td>301803-30180002</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-01-31T16:00:00.000Z</td>\n",
       "      <td>10436.0</td>\n",
       "      <td>2025-02-28T16:00:00.000Z</td>\n",
       "      <td>10272.0</td>\n",
       "      <td>10446.0</td>\n",
       "      <td>-0.238755</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12331.081081</td>\n",
       "      <td>2067.081081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234143 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id_trans region_id_trans region_name_trans  \\\n",
       "0                   I20250301813              10  Hong Kong Island   \n",
       "1                   I20250301813              10  Hong Kong Island   \n",
       "2                   I20250302773              10  Hong Kong Island   \n",
       "3                   I20250400203              10  Hong Kong Island   \n",
       "4                   I20250301892              10  Hong Kong Island   \n",
       "...                          ...             ...               ...   \n",
       "234138  NO2021050521050501840081              30   New Territories   \n",
       "234139  NO2021050421050401950028              30   New Territories   \n",
       "234140  NO2021050321050301110058              30   New Territories   \n",
       "234141  NO2021043021043002190055              30   New Territories   \n",
       "234142  NO2021043021043000550128              30   New Territories   \n",
       "\n",
       "       subregion_id_trans            subregion district_id_trans  \\\n",
       "0                    1001  Central And Western            100103   \n",
       "1                    1001  Central And Western            100103   \n",
       "2                    1001  Central And Western            100103   \n",
       "3                    1001  Central And Western            100103   \n",
       "4                    1001  Central And Western            100103   \n",
       "...                   ...                  ...               ...   \n",
       "234138               3018              Islands            301803   \n",
       "234139               3018              Islands            301803   \n",
       "234140               3018              Islands            301803   \n",
       "234141               3018              Islands            301803   \n",
       "234142               3018              Islands            301803   \n",
       "\n",
       "               district sm_district_id_trans         sm_district  \\\n",
       "0       Mid-Levels West             10010001        Kennedy Town   \n",
       "1       Mid-Levels West             10010001        Kennedy Town   \n",
       "2       Mid-Levels West             10010005     Mid-Levels West   \n",
       "3       Mid-Levels West             10010006  Mid-Levels Central   \n",
       "4       Mid-Levels West             10010005     Mid-Levels West   \n",
       "...                 ...                  ...                 ...   \n",
       "234138       Tung Chung             30180002          Tung Chung   \n",
       "234139       Tung Chung             30180002          Tung Chung   \n",
       "234140       Tung Chung             30180002          Tung Chung   \n",
       "234141       Tung Chung             30180002          Tung Chung   \n",
       "234142       Tung Chung             30180002          Tung Chung   \n",
       "\n",
       "       combined_district_id_trans  ... market_stat_monthly_10_date  \\\n",
       "0                 100103-10010001  ...    2025-01-31T16:00:00.000Z   \n",
       "1                 100103-10010001  ...    2025-01-31T16:00:00.000Z   \n",
       "2                 100103-10010005  ...    2025-01-31T16:00:00.000Z   \n",
       "3                 100103-10010006  ...    2025-01-31T16:00:00.000Z   \n",
       "4                 100103-10010005  ...    2025-01-31T16:00:00.000Z   \n",
       "...                           ...  ...                         ...   \n",
       "234138            301803-30180002  ...    2025-01-31T16:00:00.000Z   \n",
       "234139            301803-30180002  ...    2025-01-31T16:00:00.000Z   \n",
       "234140            301803-30180002  ...    2025-01-31T16:00:00.000Z   \n",
       "234141            301803-30180002  ...    2025-01-31T16:00:00.000Z   \n",
       "234142            301803-30180002  ...    2025-01-31T16:00:00.000Z   \n",
       "\n",
       "       market_stat_monthly_10_avg_net_ft_price market_stat_monthly_11_date  \\\n",
       "0                                      19933.0    2025-02-28T16:00:00.000Z   \n",
       "1                                      19933.0    2025-02-28T16:00:00.000Z   \n",
       "2                                      15753.0    2025-02-28T16:00:00.000Z   \n",
       "3                                      16334.0    2025-02-28T16:00:00.000Z   \n",
       "4                                      22373.0    2025-02-28T16:00:00.000Z   \n",
       "...                                        ...                         ...   \n",
       "234138                                  9990.0    2025-02-28T16:00:00.000Z   \n",
       "234139                                  9990.0    2025-02-28T16:00:00.000Z   \n",
       "234140                                 10644.0    2025-02-28T16:00:00.000Z   \n",
       "234141                                 10644.0    2025-02-28T16:00:00.000Z   \n",
       "234142                                 10436.0    2025-02-28T16:00:00.000Z   \n",
       "\n",
       "       market_stat_monthly_11_avg_net_ft_price  \\\n",
       "0                                      19478.0   \n",
       "1                                      19478.0   \n",
       "2                                      15753.0   \n",
       "3                                      16334.0   \n",
       "4                                      22373.0   \n",
       "...                                        ...   \n",
       "234138                                 11083.0   \n",
       "234139                                 11083.0   \n",
       "234140                                 11353.0   \n",
       "234141                                 11353.0   \n",
       "234142                                 10272.0   \n",
       "\n",
       "       index_component_estate_net_ft_price  \\\n",
       "0                                  19449.0   \n",
       "1                                  19449.0   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "...                                    ...   \n",
       "234138                             11052.0   \n",
       "234139                             11052.0   \n",
       "234140                             11127.0   \n",
       "234141                             11127.0   \n",
       "234142                             10446.0   \n",
       "\n",
       "       index_component_estate_net_ft_price_chg parent_estate_id  \\\n",
       "0                                    -0.025702                    \n",
       "1                                    -0.025702                    \n",
       "2                                          NaN                    \n",
       "3                                          NaN                    \n",
       "4                                          NaN                    \n",
       "...                                        ...              ...   \n",
       "234138                               -3.711448                    \n",
       "234139                               -3.711448                    \n",
       "234140                               -0.233121                    \n",
       "234141                               -0.233121                    \n",
       "234142                               -0.238755                    \n",
       "\n",
       "       parent_estate_name price_per_sqft price_variance  \n",
       "0                              48.834628  -20122.165372  \n",
       "1                              48.834628  -20122.165372  \n",
       "2                              49.315068  -15703.684932  \n",
       "3                           18372.703412     611.703412  \n",
       "4                           18118.811881      -0.188119  \n",
       "...                   ...            ...            ...  \n",
       "234138                      12808.022923    1460.022923  \n",
       "234139                      15463.917526    4115.917526  \n",
       "234140                      13141.524106    1833.524106  \n",
       "234141                      12954.898911    1646.898911  \n",
       "234142                      12331.081081    2067.081081  \n",
       "\n",
       "[234143 rows x 115 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pq_1_path = 'data/02_intermediate/centaline_estate_lv_1.parquet'\n",
    "#pq_2_path = 'data/03_intermediate/centaline_estate_lv_2.parquet'\n",
    "\n",
    "pq_2_path = 'data/03_primary/midland_res_trans_lv_2.parquet'\n",
    "pq_file = pd.read_parquet(pq_2_path)\n",
    "\n",
    "pq_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7955632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Subdistrict    Code  Row Count\n",
      "39  Kennedy Town  HMA111        199\n",
      "       Code  Row Count\n",
      "100  HMA111        199\n"
     ]
    }
   ],
   "source": [
    "# Grouping by 'Code' and calculating row count\n",
    "row_count_by_code_1 = pq_file.groupby(['Subdistrict', 'Code']).size().reset_index(name='Row Count')\n",
    "row_count_by_code_2 = pq_file.groupby(['Code']).size().reset_index(name='Row Count')\n",
    "\n",
    "\n",
    "# Displaying the result HMA151\n",
    "print(row_count_by_code_1[row_count_by_code_1['Code']=='HMA111'])\n",
    "print(row_count_by_code_2[row_count_by_code_2['Code']=='HMA111'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d4ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pq_path = 'data/02_intermediate/centanet_oir_buildings.parquet'\n",
    "\n",
    "pq_file = pd.read_parquet(pq_path)\n",
    "\n",
    "pq_file.to_csv('centanet_oir_buildings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3325470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:12:39,697 - INFO - Successfully loaded 17 districts from data/01_raw/midland_ici_area_code.csv\n",
      "2025-04-14 08:12:39,698 - INFO - Filtered to 16 districts (excluding 'All Districts')\n",
      "Scraping progress: 100%|██████████| 48/48 [01:00<00:00,  1.26s/it] , District=Wong Tai Sin District, Type=Shop            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Found 970 buildings across all districts and property types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "def scrape_midland_buildings(\n",
    "    csv_path: str, \n",
    "    output_path: str = \"midland_buildings.csv\",\n",
    "    request_delay: float = 0.5,\n",
    "    max_retries: int = 3,\n",
    "    log_level: int = logging.INFO,\n",
    "    save_incremental: bool = True,\n",
    "    incremental_save_frequency: int = 5,\n",
    "    resume_from_existing: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape building information from Midland ICI GraphQL API for all districts\n",
    "    and property types (Industrial, Office, Shop).\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing district IDs\n",
    "        output_path (str): Path where the output CSV will be saved\n",
    "        request_delay (float): Delay between requests in seconds to avoid rate limiting\n",
    "        max_retries (int): Maximum number of retries for failed requests\n",
    "        log_level (int): Logging level (e.g., logging.INFO, logging.DEBUG)\n",
    "        save_incremental (bool): Whether to save incremental results\n",
    "        incremental_save_frequency (int): How often to save incremental results\n",
    "        resume_from_existing (bool): Whether to resume from existing output file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all scraped building information\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(\"midland_scraper\")\n",
    "    \n",
    "    # Define property types\n",
    "    property_types = {\n",
    "        \"mr_ind\": \"Industrial\",\n",
    "        \"mr_comm\": \"Office\",\n",
    "        \"mr_shop\": \"Shop\"\n",
    "    }\n",
    "    \n",
    "    # Define the GraphQL endpoint\n",
    "    url = \"https://service.midlandici.com/building/graphql\"\n",
    "    \n",
    "    # Define the headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Origin\": \"https://www.midlandici.com.hk\",\n",
    "        \"Referer\": \"https://www.midlandici.com.hk/\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Read the CSV file with district IDs\n",
    "    try:\n",
    "        districts_df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"Successfully loaded {len(districts_df)} districts from {csv_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load CSV file {csv_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter out the \"All Districts\" row (ID=0)\n",
    "    districts_df = districts_df[districts_df['ID'] != 0]\n",
    "    logger.info(f\"Filtered to {len(districts_df)} districts (excluding 'All Districts')\")\n",
    "    \n",
    "    # Check if we should resume from existing file\n",
    "    already_scraped = set()\n",
    "    existing_buildings_df = pd.DataFrame()\n",
    "    \n",
    "    if resume_from_existing and os.path.exists(output_path):\n",
    "        try:\n",
    "            existing_buildings_df = pd.read_csv(output_path)\n",
    "            logger.info(f\"Found existing output file with {len(existing_buildings_df)} buildings\")\n",
    "            \n",
    "            # Create a set of already scraped district-property type combinations\n",
    "            if 'district_id' in existing_buildings_df.columns and 'property_type_code' in existing_buildings_df.columns:\n",
    "                already_scraped = set(\n",
    "                    existing_buildings_df[['district_id', 'property_type_code']].drop_duplicates().apply(\n",
    "                        lambda x: f\"{x['district_id']}_{x['property_type_code']}\", axis=1\n",
    "                    )\n",
    "                )\n",
    "                logger.info(f\"Found {len(already_scraped)} already scraped district-property type combinations\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read existing output file: {str(e)}. Starting from scratch.\")\n",
    "    \n",
    "    # Initialize an empty list to store all building data\n",
    "    all_buildings = []\n",
    "    \n",
    "    # Keep track of how many combinations we've processed for incremental saving\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Calculate total iterations for tqdm\n",
    "    total_iterations = len(districts_df) * len(property_types)\n",
    "    \n",
    "    # Create a progress bar\n",
    "    with tqdm(\n",
    "        total=total_iterations,\n",
    "        desc=\"Scraping progress\",\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}\"\n",
    "    ) as pbar:\n",
    "        for _, district in districts_df.iterrows():\n",
    "            district_id = district['ID']\n",
    "            district_name_en = district['Name_EN']\n",
    "            district_name_cn = district['Name_CN']\n",
    "            \n",
    "            for sbu, property_type in property_types.items():\n",
    "                # Update progress bar suffix\n",
    "                pbar.set_postfix({\n",
    "                    'District': district_name_en,\n",
    "                    'Type': property_type\n",
    "                })\n",
    "                pbar.refresh()  # Force immediate update\n",
    "\n",
    "                # Check if this combination has already been scraped\n",
    "                combo_key = f\"{district_id}_{sbu}\"\n",
    "                if combo_key in already_scraped:\n",
    "                    #logger.info(f\"Skipping already scraped {property_type} in {district_name_en}\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Update progress bar description\n",
    "                #pbar.set_description(f\"Scraping {property_type} in {district_name_en}\")\n",
    "                \n",
    "                # Define the GraphQL query and variables\n",
    "                payload = {\n",
    "                    \"query\": \"\"\"\n",
    "                        query ($districtId: ID, $query: String, $sbu: String) {\n",
    "                          buildings(districtId: $districtId, nameSearch: $query, sbu: $sbu) {\n",
    "                            sbu\n",
    "                            id\n",
    "                            nameEn\n",
    "                            nameZh\n",
    "                            addressEn\n",
    "                            addressZh\n",
    "                            __typename\n",
    "                          }\n",
    "                        }\n",
    "                    \"\"\",\n",
    "                    \"variables\": {\n",
    "                        \"sbu\": sbu,\n",
    "                        \"districtId\": district_id,\n",
    "                        \"query\": \"\"\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Implement retry mechanism\n",
    "                retries = 0\n",
    "                success = False\n",
    "                \n",
    "                while retries < max_retries and not success:\n",
    "                    try:\n",
    "                        # Make the POST request\n",
    "                        response = requests.post(url, json=payload, headers=headers)\n",
    "                        \n",
    "                        # Check if the request was successful\n",
    "                        if response.status_code == 200:\n",
    "                            data = response.json()\n",
    "                            \n",
    "                            # Check if there are buildings in the response\n",
    "                            if 'data' in data and 'buildings' in data['data']:\n",
    "                                buildings = data['data']['buildings']\n",
    "                                \n",
    "                                if buildings:\n",
    "                                    #logger.info(f\"Found {len(buildings)} {property_type} buildings in {district_name_en}\")\n",
    "                                    \n",
    "                                    # Add district and property type information to each building\n",
    "                                    for building in buildings:\n",
    "                                        building['district_id'] = district_id\n",
    "                                        building['district_name_en'] = district_name_en\n",
    "                                        building['district_name_cn'] = district_name_cn\n",
    "                                        building['property_type'] = property_type\n",
    "                                        building['property_type_code'] = sbu\n",
    "                                        \n",
    "                                        # Add to the list of all buildings\n",
    "                                        all_buildings.append(building)\n",
    "                                #else:\n",
    "                                    #logger.info(f\"No {property_type} buildings found in {district_name_en}\")\n",
    "                                \n",
    "                                success = True\n",
    "                            else:\n",
    "                                logger.warning(f\"Unexpected response structure for {district_name_en}, {property_type}\")\n",
    "                                retries += 1\n",
    "                        else:\n",
    "                            logger.warning(f\"Request failed for {district_name_en}, {property_type} with status code {response.status_code}\")\n",
    "                            retries += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error occurred for {district_name_en}, {property_type}: {str(e)}\")\n",
    "                        retries += 1\n",
    "                    \n",
    "                    # If this isn't the last retry and we haven't succeeded, wait before retrying\n",
    "                    if retries < max_retries and not success:\n",
    "                        time.sleep(request_delay * 2)  # Longer delay for retries\n",
    "                \n",
    "                # Update processed count\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Save incremental results if needed\n",
    "                if save_incremental and processed_count % incremental_save_frequency == 0:\n",
    "                    _save_incremental_results(\n",
    "                        all_buildings, existing_buildings_df, output_path, logger\n",
    "                    )\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(request_delay)\n",
    "    \n",
    "    # Convert the list of buildings to a DataFrame\n",
    "    buildings_df = _process_and_save_results(\n",
    "        all_buildings, existing_buildings_df, output_path, logger\n",
    "    )\n",
    "    \n",
    "    return buildings_df\n",
    "\n",
    "def _process_and_save_results(\n",
    "    new_buildings: List[Dict[str, Any]], \n",
    "    existing_buildings_df: pd.DataFrame,\n",
    "    output_path: str,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process new buildings data, combine with existing data if any, and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        new_buildings: List of new building dictionaries\n",
    "        existing_buildings_df: DataFrame of existing buildings (can be empty)\n",
    "        output_path: Path to save the final CSV\n",
    "        logger: Logger instance\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame of all buildings\n",
    "    \"\"\"\n",
    "    if not new_buildings and existing_buildings_df.empty:\n",
    "        logger.warning(\"No buildings found in any district for any property type\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert new buildings to DataFrame\n",
    "    if new_buildings:\n",
    "        new_buildings_df = pd.DataFrame(new_buildings)\n",
    "        \n",
    "        # Add suffix information for clarity\n",
    "        new_buildings_df['district_property_type'] = new_buildings_df.apply(\n",
    "            lambda row: f\"{row['district_name_en']}_{row['property_type']}\", axis=1\n",
    "        )\n",
    "    else:\n",
    "        new_buildings_df = pd.DataFrame()\n",
    "    \n",
    "    # Combine with existing data if there is any\n",
    "    if not existing_buildings_df.empty and not new_buildings_df.empty:\n",
    "        # Ensure 'district_property_type' exists in existing data\n",
    "        if 'district_property_type' not in existing_buildings_df.columns:\n",
    "            existing_buildings_df['district_property_type'] = existing_buildings_df.apply(\n",
    "                lambda row: f\"{row['district_name_en']}_{row['property_type']}\" \n",
    "                if 'district_name_en' in existing_buildings_df.columns and 'property_type' in existing_buildings_df.columns \n",
    "                else \"\", axis=1\n",
    "            )\n",
    "        \n",
    "        # Combine DataFrames\n",
    "        combined_df = pd.concat([existing_buildings_df, new_buildings_df], ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on building ID and property type\n",
    "        if 'id' in combined_df.columns and 'property_type_code' in combined_df.columns:\n",
    "            combined_df = combined_df.drop_duplicates(subset=['id', 'property_type_code'])\n",
    "    elif not new_buildings_df.empty:\n",
    "        combined_df = new_buildings_df\n",
    "    else:\n",
    "        combined_df = existing_buildings_df\n",
    "    \n",
    "    # Save to CSV\n",
    "    #try:\n",
    "    #    combined_df.to_csv(output_path, index=False)\n",
    "        #logger.info(f\"Successfully saved {len(combined_df)} buildings to {output_path}\")\n",
    "    #except Exception as e:\n",
    "    #    logger.error(f\"Failed to save CSV file: {str(e)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def _save_incremental_results(\n",
    "    new_buildings: List[Dict[str, Any]], \n",
    "    existing_buildings_df: pd.DataFrame,\n",
    "    output_path: str,\n",
    "    logger: logging.Logger\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save incremental results to avoid data loss if the process is interrupted.\n",
    "    \n",
    "    Args:\n",
    "        new_buildings: List of new building dictionaries\n",
    "        existing_buildings_df: DataFrame of existing buildings (can be empty)\n",
    "        output_path: Path to save the incremental CSV\n",
    "        logger: Logger instance\n",
    "    \"\"\"\n",
    "    if not new_buildings:\n",
    "        logger.debug(\"No new buildings to save incrementally\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Process and save results\n",
    "        _process_and_save_results(\n",
    "            new_buildings, existing_buildings_df, output_path, logger\n",
    "        )\n",
    "        #logger.info(f\"Saved incremental results with {len(new_buildings)} new buildings\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save incremental results: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual CSV path\n",
    "    csv_path = \"data/01_raw/midland_ici_area_code.csv\"\n",
    "    output_path = \"midland_buildings.csv\"\n",
    "    \n",
    "    # Run the scraper\n",
    "    buildings_df = scrape_midland_buildings(\n",
    "        csv_path=csv_path,\n",
    "        output_path=output_path,\n",
    "        request_delay=0.5,  # Adjust as needed to avoid rate limiting\n",
    "        max_retries=3,\n",
    "        log_level=logging.INFO,\n",
    "        save_incremental=True,\n",
    "        incremental_save_frequency=5,\n",
    "        resume_from_existing=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Scraping completed. Found {len(buildings_df)} buildings across all districts and property types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b72e3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_df.to_csv('midland_buildings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2729bee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbu</th>\n",
       "      <th>id</th>\n",
       "      <th>nameEn</th>\n",
       "      <th>nameZh</th>\n",
       "      <th>addressEn</th>\n",
       "      <th>addressZh</th>\n",
       "      <th>__typename</th>\n",
       "      <th>district_id</th>\n",
       "      <th>district_name_en</th>\n",
       "      <th>district_name_cn</th>\n",
       "      <th>property_type</th>\n",
       "      <th>property_type_code</th>\n",
       "      <th>district_property_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr_ind</td>\n",
       "      <td>B000025468</td>\n",
       "      <td>YIUGA FTY BLDG</td>\n",
       "      <td>耀基工廈</td>\n",
       "      <td>62 VICTORIA ROAD</td>\n",
       "      <td>域多利道62號</td>\n",
       "      <td>Building</td>\n",
       "      <td>1</td>\n",
       "      <td>Central and Western District</td>\n",
       "      <td>中西區</td>\n",
       "      <td>Industrial</td>\n",
       "      <td>mr_ind</td>\n",
       "      <td>Central and Western District_Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000115397</td>\n",
       "      <td>WOHO</td>\n",
       "      <td>WOHO</td>\n",
       "      <td>119 Jervois Street</td>\n",
       "      <td>蘇杭街119號</td>\n",
       "      <td>Building</td>\n",
       "      <td>1</td>\n",
       "      <td>Central and Western District</td>\n",
       "      <td>中西區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Central and Western District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000112477</td>\n",
       "      <td>The Henderson</td>\n",
       "      <td>The Henderson</td>\n",
       "      <td>2 Murray Road</td>\n",
       "      <td>美利道2號</td>\n",
       "      <td>Building</td>\n",
       "      <td>1</td>\n",
       "      <td>Central and Western District</td>\n",
       "      <td>中西區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Central and Western District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000089563</td>\n",
       "      <td>L L Tower</td>\n",
       "      <td>L L Tower</td>\n",
       "      <td>2-4 Shelley Street</td>\n",
       "      <td>些利街2-4號</td>\n",
       "      <td>Building</td>\n",
       "      <td>1</td>\n",
       "      <td>Central and Western District</td>\n",
       "      <td>中西區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Central and Western District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000058939</td>\n",
       "      <td>Two International Finance Centre</td>\n",
       "      <td>國際金融中心二期</td>\n",
       "      <td>8 Finance Street</td>\n",
       "      <td>金融街8號</td>\n",
       "      <td>Building</td>\n",
       "      <td>1</td>\n",
       "      <td>Central and Western District</td>\n",
       "      <td>中西區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Central and Western District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000111508</td>\n",
       "      <td>The Burrow</td>\n",
       "      <td>The Burrow</td>\n",
       "      <td>212 Choi Hung Road</td>\n",
       "      <td>彩虹道212號</td>\n",
       "      <td>Building</td>\n",
       "      <td>8</td>\n",
       "      <td>Wong Tai Sin District</td>\n",
       "      <td>黃大仙區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Wong Tai Sin District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000080050</td>\n",
       "      <td>On Tin Centre</td>\n",
       "      <td>安田中心</td>\n",
       "      <td>1-3 Sheung Hei Street</td>\n",
       "      <td>雙喜街1-3號</td>\n",
       "      <td>Building</td>\n",
       "      <td>8</td>\n",
       "      <td>Wong Tai Sin District</td>\n",
       "      <td>黃大仙區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Wong Tai Sin District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000059457</td>\n",
       "      <td>One Portside</td>\n",
       "      <td>宏基中心一期</td>\n",
       "      <td>29 Tai Yau Street</td>\n",
       "      <td>大有街29號</td>\n",
       "      <td>Building</td>\n",
       "      <td>8</td>\n",
       "      <td>Wong Tai Sin District</td>\n",
       "      <td>黃大仙區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Wong Tai Sin District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000080611</td>\n",
       "      <td>Maxgrand Plaza</td>\n",
       "      <td>萬迪廣場</td>\n",
       "      <td>3 Tai Yau Street</td>\n",
       "      <td>大有街3號</td>\n",
       "      <td>Building</td>\n",
       "      <td>8</td>\n",
       "      <td>Wong Tai Sin District</td>\n",
       "      <td>黃大仙區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Wong Tai Sin District_Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>mr_comm</td>\n",
       "      <td>B000066796</td>\n",
       "      <td>Win Plaza</td>\n",
       "      <td>匯達商業中心</td>\n",
       "      <td>9 Sheung Hei Street</td>\n",
       "      <td>雙喜街9號</td>\n",
       "      <td>Building</td>\n",
       "      <td>8</td>\n",
       "      <td>Wong Tai Sin District</td>\n",
       "      <td>黃大仙區</td>\n",
       "      <td>Office</td>\n",
       "      <td>mr_comm</td>\n",
       "      <td>Wong Tai Sin District_Office</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sbu          id                            nameEn         nameZh  \\\n",
       "0     mr_ind  B000025468                    YIUGA FTY BLDG           耀基工廈   \n",
       "1    mr_comm  B000115397                              WOHO           WOHO   \n",
       "2    mr_comm  B000112477                     The Henderson  The Henderson   \n",
       "3    mr_comm  B000089563                         L L Tower      L L Tower   \n",
       "4    mr_comm  B000058939  Two International Finance Centre       國際金融中心二期   \n",
       "..       ...         ...                               ...            ...   \n",
       "965  mr_comm  B000111508                        The Burrow     The Burrow   \n",
       "966  mr_comm  B000080050                     On Tin Centre           安田中心   \n",
       "967  mr_comm  B000059457                      One Portside         宏基中心一期   \n",
       "968  mr_comm  B000080611                    Maxgrand Plaza           萬迪廣場   \n",
       "969  mr_comm  B000066796                         Win Plaza         匯達商業中心   \n",
       "\n",
       "                 addressEn addressZh __typename  district_id  \\\n",
       "0         62 VICTORIA ROAD   域多利道62號   Building            1   \n",
       "1       119 Jervois Street   蘇杭街119號   Building            1   \n",
       "2            2 Murray Road     美利道2號   Building            1   \n",
       "3       2-4 Shelley Street   些利街2-4號   Building            1   \n",
       "4         8 Finance Street     金融街8號   Building            1   \n",
       "..                     ...       ...        ...          ...   \n",
       "965     212 Choi Hung Road   彩虹道212號   Building            8   \n",
       "966  1-3 Sheung Hei Street   雙喜街1-3號   Building            8   \n",
       "967      29 Tai Yau Street    大有街29號   Building            8   \n",
       "968       3 Tai Yau Street     大有街3號   Building            8   \n",
       "969    9 Sheung Hei Street     雙喜街9號   Building            8   \n",
       "\n",
       "                 district_name_en district_name_cn property_type  \\\n",
       "0    Central and Western District              中西區    Industrial   \n",
       "1    Central and Western District              中西區        Office   \n",
       "2    Central and Western District              中西區        Office   \n",
       "3    Central and Western District              中西區        Office   \n",
       "4    Central and Western District              中西區        Office   \n",
       "..                            ...              ...           ...   \n",
       "965         Wong Tai Sin District             黃大仙區        Office   \n",
       "966         Wong Tai Sin District             黃大仙區        Office   \n",
       "967         Wong Tai Sin District             黃大仙區        Office   \n",
       "968         Wong Tai Sin District             黃大仙區        Office   \n",
       "969         Wong Tai Sin District             黃大仙區        Office   \n",
       "\n",
       "    property_type_code                   district_property_type  \n",
       "0               mr_ind  Central and Western District_Industrial  \n",
       "1              mr_comm      Central and Western District_Office  \n",
       "2              mr_comm      Central and Western District_Office  \n",
       "3              mr_comm      Central and Western District_Office  \n",
       "4              mr_comm      Central and Western District_Office  \n",
       "..                 ...                                      ...  \n",
       "965            mr_comm             Wong Tai Sin District_Office  \n",
       "966            mr_comm             Wong Tai Sin District_Office  \n",
       "967            mr_comm             Wong Tai Sin District_Office  \n",
       "968            mr_comm             Wong Tai Sin District_Office  \n",
       "969            mr_comm             Wong Tai Sin District_Office  \n",
       "\n",
       "[970 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Automatically installs and initializes ChromeDriver in headless mode\"\"\"\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def detect_and_replace_svg(icon_url):\n",
    "    \"\"\"Convert SVG icons to Yes/No values\"\"\"\n",
    "    if 'PD94bW' in icon_url and \"jUwl\" in icon_url:\n",
    "        return \"Yes\"\n",
    "    elif 'PD94bW' in icon_url and \"gNTAi\" in icon_url:\n",
    "        return \"No\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def scrape_building_info(row, driver):\n",
    "    \"\"\"Scrape detailed information for individual buildings\"\"\"\n",
    "    try:\n",
    "        url = row['Detail URL'] + \"?lang=english\"\n",
    "        driver.get(url)\n",
    "        time.sleep(1.5)  # Reduced sleep for faster scraping\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        building_info = {\n",
    "            'Building Name': row['Building Name'],\n",
    "            'Address': row['Address'],\n",
    "            'URL': url\n",
    "        }\n",
    "        \n",
    "        # Extract meta information blocks\n",
    "        for block in soup.find_all('div', class_='meta-info-container'):\n",
    "            title = block.find('div', class_='title')\n",
    "            content = block.find('div', class_='content')\n",
    "            icon = block.find('div', class_='icon')\n",
    "            \n",
    "            if title and (content or icon):\n",
    "                key = title.text.strip()\n",
    "                value = content.text.strip() if content else detect_and_replace_svg(icon['src'])\n",
    "                building_info[key] = value\n",
    "                \n",
    "        return building_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {row['Building Name']}: {str(e)[:100]}...\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Scrape detailed information\n",
    "print(\"\\nStarting detailed information scraping...\")\n",
    "driver = initialize_driver()\n",
    "df = pd.read_csv(\"midlandici_building_list.csv\")\n",
    "results = []\n",
    "\n",
    "try:\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing buildings\"):\n",
    "        result = scrape_building_info(row, driver)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            # Progressive save every 5 entries\n",
    "            if len(results) % 5 == 0:\n",
    "                pd.DataFrame(results).to_csv(\"midlandici_detailed_info.csv\", index=False)\n",
    "                \n",
    "    # Final save\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(\"midlandici_detailed_info.csv\", index=False)\n",
    "        print(f\"\\nSuccessfully scraped {len(results)}/{len(df)} buildings\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nUser interrupted! Saving current progress...\")\n",
    "    pd.DataFrame(results).to_csv(\"midlandici_detailed_info_PARTIAL.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9bea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape for Building Details\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Automatically installs and initializes ChromeDriver in headless mode\"\"\"\n",
    "    chromedriver_autoinstaller.install()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def detect_and_replace_svg(icon_url):\n",
    "    \"\"\"Convert SVG icons to Yes/No values\"\"\"\n",
    "    if icon_url and 'PD94bW' in icon_url:\n",
    "        if \"jUwl\" in icon_url:\n",
    "            return \"Yes\"\n",
    "        elif \"gNTAi\" in icon_url:\n",
    "            return \"No\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def clean_for_url(text):\n",
    "    \"\"\"Clean text for use in URL\"\"\"\n",
    "    # Replace spaces with hyphens\n",
    "    text = text.replace(' ', '-')\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\-]', '', text)\n",
    "    # Remove consecutive hyphens\n",
    "    text = re.sub(r'-+', '-', text)\n",
    "    return text\n",
    "\n",
    "def construct_url(row):\n",
    "    \"\"\"Construct the URL for detailed scraping based on building information\"\"\"\n",
    "    building_type = row['__typename'].lower()\n",
    "    building_id = row['id']\n",
    "    building_name = clean_for_url(row['nameEn'])\n",
    "    return f\"https://www.midlandici.com.hk/ics/property/{building_type}/details/{building_id}/{building_name}?lang=english\"\n",
    "\n",
    "def scrape_building_info(row, driver):\n",
    "    \"\"\"Scrape detailed information for individual buildings\"\"\"\n",
    "    try:\n",
    "        url = construct_url(row)\n",
    "        driver.get(url)\n",
    "        time.sleep(1.5)  # Sleep for 1.5 seconds to ensure page loads\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        building_info = {\n",
    "            'id': row['id'],\n",
    "            'Building Name': row['nameEn'],\n",
    "            'Building Name (Chinese)': row['nameZh'],\n",
    "            'Address': row['addressEn'],\n",
    "            'Address (Chinese)': row['addressZh'],\n",
    "            'District': row['district_name_en'],\n",
    "            'Property Type': row['property_type'],\n",
    "            'URL': url\n",
    "        }\n",
    "        \n",
    "        # Extract meta information blocks\n",
    "        for block in soup.find_all('div', class_='meta-info-container'):\n",
    "            title = block.find('div', class_='title')\n",
    "            content = block.find('div', class_='content')\n",
    "            icon = block.find('div', class_='icon')\n",
    "            \n",
    "            if title and (content or icon):\n",
    "                key = title.text.strip()\n",
    "                if content:\n",
    "                    value = content.text.strip()\n",
    "                elif icon and 'src' in icon.attrs:\n",
    "                    value = detect_and_replace_svg(icon['src'])\n",
    "                else:\n",
    "                    value = \"Unknown\"\n",
    "                building_info[key] = value\n",
    "                \n",
    "        return building_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {row['nameEn']}: {str(e)[:100]}...\")\n",
    "        return None\n",
    "\n",
    "def scrape_buildings(input_csv, output_file):\n",
    "    \"\"\"\n",
    "    Scrape detailed information for buildings listed in input_csv and save to output_file.\n",
    "    Skip buildings that are already in the output file.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file\n",
    "        output_file (str): Path to the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the scraped information\n",
    "    \"\"\"\n",
    "    # Read input CSV\n",
    "    df_input = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Check if output file exists, and read it if it does\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        df_output = pd.read_csv(output_file)\n",
    "        if 'id' in df_output.columns:\n",
    "            existing_ids = set(df_output['id'])\n",
    "    \n",
    "    # Filter out buildings that are already in the output file\n",
    "    df_to_scrape = df_input[~df_input['id'].isin(existing_ids)]\n",
    "    \n",
    "    if len(df_to_scrape) == 0:\n",
    "        print(\"All buildings are already scraped. Nothing to do.\")\n",
    "        if os.path.exists(output_file):\n",
    "            return pd.read_csv(output_file)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = initialize_driver()\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping detailed information for {len(df_to_scrape)} buildings...\")\n",
    "        for index, row in tqdm(df_to_scrape[:10].iterrows(), total=len(df_to_scrape), desc=\"Processing buildings\"):\n",
    "            result = scrape_building_info(row, driver)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        # Create DataFrame from results\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # If output file exists, append new results to it\n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "            df_existing = pd.read_csv(output_file)\n",
    "            df_combined = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "            df_combined.to_csv(output_file, index=False)\n",
    "            print(f\"Added {len(df_results)} new buildings to {output_file}\")\n",
    "            return df_combined\n",
    "        else:\n",
    "            # Save results to output file\n",
    "            df_results.to_csv(output_file, index=False)\n",
    "            print(f\"Saved {len(df_results)} buildings to {output_file}\")\n",
    "            return df_results\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b142bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:28:51,693 - INFO - Chromedriver is already installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping detailed information for 960 buildings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings:   1%|          | 10/960 [00:24<38:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 new buildings to midland_detailed_info.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the example if script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your input and output file paths\n",
    "    input_file = \"midland_buildings.csv\"\n",
    "    output_file = \"midland_detailed_info.csv\"\n",
    "    \n",
    "    # Run the scraper\n",
    "    results_df = scrape_buildings(input_file, output_file)\n",
    "    \n",
    "    results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fafa9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing buildings:   8%|▊         | 72/950 [01:39<20:13,  1.38s/building, Building: Effectual Building]                  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m input_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidland_buildings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidland_detailed_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_buildings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 89\u001b[0m, in \u001b[0;36mprocess_buildings\u001b[0;34m(input_csv, output_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_to_scrape\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Update tqdm suffix with building name only\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnameEn\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_with_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     91\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m, in \u001b[0;36mscrape_with_requests\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m construct_url(row)\n\u001b[1;32m     23\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     29\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try Get Method\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "def clean_for_url(text):\n",
    "    \"\"\"Clean text for URL formatting by removing apostrophes and replacing spaces with hyphens.\"\"\"\n",
    "    text = text.replace(\"'\", \"\")  # Remove apostrophes\n",
    "    return '-'.join(text.split()).strip()\n",
    "\n",
    "def construct_url(row):\n",
    "    \"\"\"Construct the URL for detailed scraping based on building information.\"\"\"\n",
    "    return f\"https://www.midlandici.com.hk/ics/property/{row['__typename'].lower()}/details/{row['id']}/{clean_for_url(row['nameEn'])}?lang=english\"\n",
    "\n",
    "def scrape_with_requests(row):\n",
    "    \"\"\"Scrape building details using requests library.\"\"\"\n",
    "    try:\n",
    "        url = construct_url(row)\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        building_info = {\n",
    "            'id': row['id'],\n",
    "            'Building Name': row['nameEn'],\n",
    "            'URL': url\n",
    "        }\n",
    "\n",
    "        # Extract meta information\n",
    "        for block in soup.find_all('div', class_='meta-info-container'):\n",
    "            title = block.find('div', class_='title')\n",
    "            content = block.find('div', class_='content')\n",
    "            \n",
    "            if title and content:\n",
    "                key = title.text.strip()\n",
    "                value = content.text.strip()\n",
    "                building_info[key] = value\n",
    "                \n",
    "        return building_info\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for {row['nameEn']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_buildings(input_csv, output_file):\n",
    "    \"\"\"\n",
    "    Process buildings with GET requests and save results to output file.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the scraped information.\n",
    "    \"\"\"\n",
    "    # Read input CSV\n",
    "    df_input = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Check existing data\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        existing_ids = set(existing_df['id']) if 'id' in existing_df.columns else set()\n",
    "    \n",
    "    # Filter out buildings already in the output file\n",
    "    df_to_scrape = df_input[~df_input['id'].isin(existing_ids)]\n",
    "    \n",
    "    if len(df_to_scrape) == 0:\n",
    "        print(\"All buildings are already scraped. Nothing to do.\")\n",
    "        if os.path.exists(output_file):\n",
    "            return pd.read_csv(output_file)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process buildings with tqdm progress bar and dynamic suffix\n",
    "    with tqdm(total=len(df_to_scrape), desc=\"Processing buildings\", unit=\"building\") as pbar:\n",
    "        for _, row in df_to_scrape.iterrows():\n",
    "            # Update tqdm suffix with building name only\n",
    "            pbar.set_postfix_str(f\"Building: {row['nameEn']}\")\n",
    "            \n",
    "            result = scrape_with_requests(row)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "            \n",
    "            # Add a small delay to avoid overwhelming the server\n",
    "            time.sleep(1)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save results to output file\n",
    "    if results:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Append new data to existing file or create a new one\n",
    "        if os.path.exists(output_file):\n",
    "            df_existing = pd.read_csv(output_file)\n",
    "            df_combined = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "            df_combined.to_csv(output_file, index=False)\n",
    "            print(f\"Added {len(df_results)} new records to {output_file}\")\n",
    "            return df_combined\n",
    "        else:\n",
    "            df_results.to_csv(output_file, index=False)\n",
    "            print(f\"Saved {len(df_results)} buildings to {output_file}\")\n",
    "            return df_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path = \"midland_buildings.csv\"\n",
    "    output_csv_path = \"midland_detailed_info.csv\"\n",
    "    \n",
    "    result_df = process_buildings(input_csv_path, output_csv_path)\n",
    "    \n",
    "    print(\"\\nScraping complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552aff04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
